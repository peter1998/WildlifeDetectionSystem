import os
import json
import pandas as pd
import numpy as np
from datetime import datetime
from flask import current_app
from app import db
from app.models.models import Species, Annotation, Image
import logging

class ModelPerformanceService:
    """Service for tracking and analyzing model performance of wildlife detection models."""
    
    @staticmethod
    def _find_latest_model_path():
        """
        Helper method to find the most recent model directory.
        Returns tuple of (model_folder_name, full_path_to_model)
        """
        # Path to trained models
        models_dir = current_app.config.get('MODEL_FOLDER', 
                    os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(
                    os.path.abspath(__file__)))), 'models', 'trained'))
        
        # Find all model folders by creation date
        model_folders = []
        try:
            for folder in os.listdir(models_dir):
                folder_path = os.path.join(models_dir, folder)
                if os.path.isdir(folder_path) and 'wildlife_detector' in folder:
                    creation_time = os.path.getctime(folder_path)
                    model_folders.append((folder, creation_time, folder_path))
            
            if not model_folders:
                logging.warning("No model folders found in %s", models_dir)
                return None, None
                
            # Sort by creation time (newest first)
            model_folders.sort(key=lambda x: x[1], reverse=True)
            return model_folders[0][0], model_folders[0][2]
        except Exception as e:
            logging.error("Error finding latest model path: %s", str(e))
            return None, None

    @staticmethod
    def get_current_model_details():
        """Get the current active model details."""
        latest_model_folder, latest_model_path = ModelPerformanceService._find_latest_model_path()
        
        if not latest_model_path:
            return None
        
        try:
            # Get model details from args.yaml file
            args = {}
            args_path = os.path.join(latest_model_path, 'args.yaml')
            
            # Try alternative configuration files if args.yaml doesn't exist
            config_files = [
                args_path,
                os.path.join(latest_model_path, 'config.yaml'),
                os.path.join(latest_model_path, 'hyp.yaml')
            ]
            
            for config_file in config_files:
                if os.path.exists(config_file):
                    with open(config_file, 'r') as f:
                        import yaml
                        args = yaml.safe_load(f)
                    break
            
            # Check for a more detailed model_config.json file (generated by Cell 8)
            model_config_path = os.path.join(latest_model_path, 'model_config.json')
            if os.path.exists(model_config_path):
                with open(model_config_path, 'r') as f:
                    config_data = json.load(f)
                    # Override args with more detailed config
                    if 'config' in config_data:
                        args = config_data['config']
            
            # Get creation date
            model_created_at = os.path.getctime(latest_model_path)
            creation_date = datetime.fromtimestamp(model_created_at).strftime('%Y-%m-%d %H:%M:%S')
            
            # Check if weights exist
            weights_paths = [
                os.path.join(latest_model_path, 'weights', 'best.pt'),
                os.path.join(latest_model_path, 'best.pt'),
                os.path.join(latest_model_path, 'weights/last.pt'),
                os.path.join(latest_model_path, 'last.pt')
            ]
            
            weights_file = 'N/A'
            for weights_path in weights_paths:
                if os.path.exists(weights_path):
                    weights_file = os.path.basename(weights_path)
                    break
            
            # Check for additional model metadata
            model_metadata = {}
            metadata_path = os.path.join(latest_model_path, 'model_metadata.json')
            if os.path.exists(metadata_path):
                with open(metadata_path, 'r') as f:
                    model_metadata = json.load(f)
            
            # Check for training dataset information
            dataset_info = {}
            dataset_info_path = os.path.join(latest_model_path, 'dataset_info.json')
            if os.path.exists(dataset_info_path):
                with open(dataset_info_path, 'r') as f:
                    dataset_info = json.load(f)
                    
            # Build complete model details
            return {
                'model_name': latest_model_folder,
                'created_at': creation_date,
                'weights_file': weights_file,
                'model_type': model_metadata.get('model_type', 'YOLOv8'),
                'image_size': model_metadata.get('image_size', args.get('imgsz', 640)),
                'config': args,
                'train_images': dataset_info.get('train_images', 'N/A'),
                'val_images': dataset_info.get('val_images', 'N/A'),
                'class_count': dataset_info.get('class_count', 'N/A'),
                'data_split': dataset_info.get('data_split', '80/20'),
                'early_stopping': args.get('patience', 'N/A'),
                'device': args.get('device', 'CPU'),
                'training_time': model_metadata.get('training_time', 'N/A')
            }
        except Exception as e:
            logging.error("Error getting model details: %s", str(e))
            return {
                'model_name': latest_model_folder,
                'created_at': 'Unknown',
                'weights_file': 'Error loading model details',
                'config': {}
            }
    
    @staticmethod
    def get_performance_metrics():
        """Get performance metrics for the current model."""
        _, latest_model_path = ModelPerformanceService._find_latest_model_path()
        
        if not latest_model_path:
            return {
                'precision': 0,
                'recall': 0,
                'mAP50': 0,
                'mAP50-95': 0,
                'per_class': {}
            }
        
        try:
            # Check for performance_summary.json (generated by Cell 8)
            summary_path = os.path.join(latest_model_path, 'performance_summary.json')
            if os.path.exists(summary_path):
                with open(summary_path, 'r') as f:
                    summary = json.load(f)
                    
                # Get per-class metrics
                per_class = {}
                class_metrics_path = os.path.join(latest_model_path, 'class_metrics.json')
                if os.path.exists(class_metrics_path):
                    with open(class_metrics_path, 'r') as f:
                        per_class = json.load(f)
                
                # Get threshold data if available
                thresholds = []
                threshold_path = os.path.join(latest_model_path, 'threshold_analysis.json')
                if os.path.exists(threshold_path):
                    with open(threshold_path, 'r') as f:
                        thresholds = json.load(f)
                
                return {
                    'precision': summary.get('precision', 0),
                    'recall': summary.get('recall', 0),
                    'mAP50': summary.get('mAP50', 0),
                    'mAP50-95': summary.get('mAP50-95', 0),
                    'training_epochs': summary.get('final_epoch', 0),
                    'best_epoch': summary.get('best_epoch', 0),
                    'per_class': per_class,
                    'thresholds': thresholds,
                    'classes': summary.get('classes', 0)
                }
            
            # Fall back to results.csv if summary doesn't exist
            return ModelPerformanceService._parse_results_csv(os.path.join(latest_model_path, 'results.csv'))
            
        except Exception as e:
            logging.error("Error getting performance metrics: %s", str(e))
            return {
                'precision': 0,
                'recall': 0,
                'mAP50': 0,
                'mAP50-95': 0,
                'error': str(e),
                'per_class': {}
            }
    
    @staticmethod
    def get_confusion_matrix():
        """Get confusion matrix for species predictions."""
        _, latest_model_path = ModelPerformanceService._find_latest_model_path()
        
        if not latest_model_path:
            return ModelPerformanceService._create_placeholder_confusion_matrix()
        
        try:
            # Check for confusion_matrix.json (generated by Cell 8)
            confusion_path = os.path.join(latest_model_path, 'confusion_matrix.json')
            if os.path.exists(confusion_path):
                with open(confusion_path, 'r') as f:
                    return json.load(f)
            
            # Check for alternative formats
            for path in [
                os.path.join(latest_model_path, 'confusion_matrix.txt'),
                os.path.join(latest_model_path, 'confusion_matrix.csv'),
                os.path.join(latest_model_path, 'confusion_matrix.npy')
            ]:
                if os.path.exists(path):
                    return ModelPerformanceService._load_confusion_matrix(path)
            
            # If no confusion matrix found, create a placeholder
            return ModelPerformanceService._create_placeholder_confusion_matrix()
            
        except Exception as e:
            logging.error("Error getting confusion matrix: %s", str(e))
            return ModelPerformanceService._create_placeholder_confusion_matrix(error=str(e))
    
    @staticmethod
    def get_recent_detection_stats():
        """Get statistics about recent model detections."""
        try:
            # Get recently verified vs. unverified annotations
            recent_annotations = Annotation.query.order_by(Annotation.created_at.desc()).limit(500).all()
            
            total = len(recent_annotations)
            verified = sum(1 for a in recent_annotations if a.is_verified)
            unverified = total - verified
            
            # Calculate correction rate (how often humans correct model predictions)
            correction_rate = 0
            corrected_count = 0
            
            # Identify species that often need correction
            species_corrections = {}
            all_species = Species.query.all()
            for s in all_species:
                species_corrections[s.name] = {
                    'total': 0,
                    'corrected': 0,
                    'correction_rate': 0
                }
                
            # Count corrections and map species IDs to names
            species_map = {s.id: s.name for s in all_species}
            
            for a in recent_annotations:
                if a.confidence is not None:  # This was a model prediction
                    species_name = species_map.get(a.species_id, "Unknown")
                    
                    if species_name in species_corrections:
                        species_corrections[species_name]['total'] += 1
                        if a.updated_at > a.created_at:  # Annotation was updated after creation
                            corrected_count += 1
                            species_corrections[species_name]['corrected'] += 1
            
            if total > 0:
                correction_rate = (corrected_count / total) * 100
            
            # Calculate per-species correction rates
            for species, counts in species_corrections.items():
                if counts['total'] > 0:
                    counts['correction_rate'] = (counts['corrected'] / counts['total']) * 100
                else:
                    counts['correction_rate'] = 0
                    
            # Filter to species with at least one detection
            filtered_species = {s: c for s, c in species_corrections.items() if c['total'] > 0}
            
            return {
                'total_recent': total,
                'verified_count': verified,
                'unverified_count': unverified,
                'correction_rate': correction_rate,
                'species_corrections': filtered_species
            }
        
        except Exception as e:
            logging.error("Error getting detection stats: %s", str(e))
            return {
                'total_recent': 0,
                'verified_count': 0,
                'unverified_count': 0,
                'correction_rate': 0,
                'species_corrections': {},
                'error': str(e)
            }
    
    @staticmethod
    def analyze_improvement_opportunities():
        """Analyze areas where the model could be improved."""
        try:
            # Get performance metrics
            performance = ModelPerformanceService.get_performance_metrics()
            detection_stats = ModelPerformanceService.get_recent_detection_stats()
            
            # Look for existing improvement analysis
            _, latest_model_path = ModelPerformanceService._find_latest_model_path()
            improvement_path = os.path.join(latest_model_path, 'improvement_analysis.json') if latest_model_path else None
            
            if improvement_path and os.path.exists(improvement_path):
                with open(improvement_path, 'r') as f:
                    return json.load(f)
            
            # Count species representation in the dataset
            species_counts = {}
            for species in Species.query.all():
                count = Annotation.query.filter_by(species_id=species.id).count()
                species_counts[species.name] = count
                
            # Find underrepresented species (fewer than 50 examples)
            underrepresented = {}
            for species, count in species_counts.items():
                if count < 50 and count > 0:  # Only include species that exist but are underrepresented
                    underrepresented[species] = count
                    
            # Find species with high correction rates
            problem_species = {}
            for species, stats in detection_stats['species_corrections'].items():
                if stats['correction_rate'] > 25 and stats['total'] > 10:  # At least 25% correction rate and 10 examples
                    problem_species[species] = stats['correction_rate']
                    
            # Look for low-performing classes based on precision/recall
            low_performing_classes = {}
            if performance and 'per_class' in performance:
                for class_name, metrics in performance['per_class'].items():
                    precision = metrics.get('precision', 0)
                    recall = metrics.get('recall', 0)
                    map50 = metrics.get('map50', 0)
                    
                    # Flag classes with low precision or recall
                    if (precision < 0.5 or recall < 0.3) and class_name in species_counts and species_counts[class_name] > 10:
                        low_performing_classes[class_name] = {
                            'precision': precision,
                            'recall': recall,
                            'map50': map50,
                            'count': species_counts.get(class_name, 0)
                        }
            
            # Get suggestions based on findings
            suggestions = []
            
            if performance.get('mAP50', 0) < 0.7:
                suggestions.append("The model's overall accuracy (mAP50) is below 70%. Consider more training data or model tuning.")
                
            if underrepresented:
                top_underrepresented = sorted(underrepresented.items(), key=lambda x: x[1])[:5]
                species_list = ', '.join([f"{s} ({c} examples)" for s, c in top_underrepresented])
                suggestions.append(f"Collect more training data for underrepresented species: {species_list}")
                
            if problem_species:
                top_problems = sorted(problem_species.items(), key=lambda x: x[1], reverse=True)[:5]
                species_list = ', '.join([f"{s} ({c:.1f}% correction rate)" for s, c in top_problems])
                suggestions.append(f"Review and improve annotations for frequently corrected species: {species_list}")
            
            if low_performing_classes:
                low_perf_list = ', '.join([f"{s} (P:{c['precision']:.2f}, R:{c['recall']:.2f})" 
                                        for s, c in list(low_performing_classes.items())[:3]])
                suggestions.append(f"Focus on improving detection for low-performing classes: {low_perf_list}")
                
            # Check for precision/recall imbalance
            if performance.get('precision', 0) > performance.get('recall', 0) + 0.2:
                suggestions.append("Model has high precision but lower recall. Consider using a lower confidence threshold for inference or augmenting training data.")
            elif performance.get('recall', 0) > performance.get('precision', 0) + 0.2:
                suggestions.append("Model has high recall but lower precision. Consider training with higher box loss weight or using a higher confidence threshold.")
                
            # Additional generic suggestions
            suggestions.append("Consider data augmentation techniques to improve model robustness.")
            suggestions.append("Evaluate model performance in different lighting conditions and environments.")
            suggestions.append("Try hierarchical detection approach with taxonomic groups for better classification.")
            
            # Get taxonomic group performance if available
            taxonomic_group_performance = {}
            if latest_model_path:
                group_path = os.path.join(latest_model_path, 'taxonomic_performance.json')
                if os.path.exists(group_path):
                    with open(group_path, 'r') as f:
                        taxonomic_group_performance = json.load(f)
            
            result = {
                'underrepresented_species': underrepresented,
                'problem_species': problem_species,
                'low_performing_classes': low_performing_classes,
                'improvement_suggestions': suggestions,
                'taxonomic_group_performance': taxonomic_group_performance
            }
            
            return result
            
        except Exception as e:
            logging.error("Error analyzing improvement opportunities: %s", str(e))
            return {
                'underrepresented_species': {},
                'problem_species': {},
                'improvement_suggestions': [
                    "Error analyzing improvement opportunities. Check logs for details.",
                    "Consider data augmentation techniques to improve model robustness.",
                    "Evaluate model performance in different lighting conditions and environments."
                ],
                'error': str(e)
            }
            
    @staticmethod
    def get_training_history():
        """Get the training history for the current model."""
        _, latest_model_path = ModelPerformanceService._find_latest_model_path()
        
        if not latest_model_path:
            return None
            
        try:
            # Check for training_history.json (generated by Cell 8)
            history_path = os.path.join(latest_model_path, 'training_history.json')
            if os.path.exists(history_path):
                with open(history_path, 'r') as f:
                    return json.load(f)
            
            # Fall back to results.csv
            results_path = os.path.join(latest_model_path, 'results.csv')
            if os.path.exists(results_path):
                results_df = pd.read_csv(results_path)
                
                # Extract key columns for history
                history_cols = ['epoch', 'box_loss', 'cls_loss', 'dfl_loss', 'precision', 'recall', 'mAP_0.5', 'mAP_0.5:0.95']
                available_cols = [col for col in history_cols if col in results_df.columns]
                
                history = {}
                for col in available_cols:
                    history[col] = results_df[col].tolist()
                
                return history
                
            return None
            
        except Exception as e:
            logging.error("Error getting training history: %s", str(e))
            return None
            
    @staticmethod
    def get_model_details_by_id(model_id):
        """Get model details for a specific model by ID."""
        # Path to trained models
        models_dir = current_app.config.get('MODEL_FOLDER', 
                    os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(
                    os.path.abspath(__file__)))), 'models', 'trained'))
        
        # Check if model exists
        model_path = os.path.join(models_dir, model_id)
        if not os.path.exists(model_path) or not os.path.isdir(model_path):
            return None
        
        try:
            # Get model details from args.yaml file
            args = {}
            args_path = os.path.join(model_path, 'args.yaml')
            
            # Try alternative configuration files if args.yaml doesn't exist
            config_files = [
                args_path,
                os.path.join(model_path, 'config.yaml'),
                os.path.join(model_path, 'hyp.yaml')
            ]
            
            for config_file in config_files:
                if os.path.exists(config_file):
                    with open(config_file, 'r') as f:
                        import yaml
                        args = yaml.safe_load(f)
                    break
            
            # Check for a more detailed model_config.json file
            model_config_path = os.path.join(model_path, 'model_config.json')
            if os.path.exists(model_config_path):
                with open(model_config_path, 'r') as f:
                    config_data = json.load(f)
                    # Override args with more detailed config
                    if 'config' in config_data:
                        args = config_data['config']
            
            # Get creation date
            model_created_at = os.path.getctime(model_path)
            creation_date = datetime.fromtimestamp(model_created_at).strftime('%Y-%m-%d %H:%M:%S')
            
            # Check for weights files
            weights_file = 'N/A'
            weights_paths = [
                os.path.join(model_path, 'weights', 'best.pt'),
                os.path.join(model_path, 'best.pt'),
                os.path.join(model_path, 'weights/last.pt'),
                os.path.join(model_path, 'last.pt')
            ]
            
            for weights_path in weights_paths:
                if os.path.exists(weights_path):
                    weights_file = os.path.basename(weights_path)
                    break
            
            # Check for additional model metadata
            model_metadata = {}
            metadata_path = os.path.join(model_path, 'model_metadata.json')
            if os.path.exists(metadata_path):
                with open(metadata_path, 'r') as f:
                    model_metadata = json.load(f)
            
            # Check for training dataset information
            dataset_info = {}
            dataset_info_path = os.path.join(model_path, 'dataset_info.json')
            if os.path.exists(dataset_info_path):
                with open(dataset_info_path, 'r') as f:
                    dataset_info = json.load(f)
            
            # Build complete model details
            return {
                'model_name': model_id,
                'created_at': creation_date,
                'weights_file': weights_file,
                'model_type': model_metadata.get('model_type', 'YOLOv8'),
                'image_size': model_metadata.get('image_size', args.get('imgsz', 640)),
                'config': args,
                'train_images': dataset_info.get('train_images', 'N/A'),
                'val_images': dataset_info.get('val_images', 'N/A'),
                'class_count': dataset_info.get('class_count', 'N/A'),
                'data_split': dataset_info.get('data_split', '80/20'),
                'early_stopping': args.get('patience', 'N/A'),
                'device': args.get('device', 'CPU'),
                'training_time': model_metadata.get('training_time', 'N/A')
            }
        except Exception as e:
            logging.error(f"Error getting model details by ID: {str(e)}")
            return {
                'model_name': model_id,
                'created_at': 'Unknown',
                'weights_file': 'Error loading model details',
                'config': {}
            }

    @staticmethod
    def get_performance_metrics_by_id(model_id):
        """Get performance metrics for a specific model by ID."""
        # Path to trained models
        models_dir = current_app.config.get('MODEL_FOLDER', 
                    os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(
                    os.path.abspath(__file__)))), 'models', 'trained'))
        
        # Check if model exists
        model_path = os.path.join(models_dir, model_id)
        if not os.path.exists(model_path) or not os.path.isdir(model_path):
            return {
                'precision': 0,
                'recall': 0,
                'mAP50': 0,
                'mAP50-95': 0,
                'per_class': {}
            }
        
        try:
            # Check for performance_summary.json
            summary_path = os.path.join(model_path, 'performance_summary.json')
            if os.path.exists(summary_path):
                with open(summary_path, 'r') as f:
                    summary = json.load(f)
                    
                # Get per-class metrics
                per_class = {}
                class_metrics_path = os.path.join(model_path, 'class_metrics.json')
                if os.path.exists(class_metrics_path):
                    with open(class_metrics_path, 'r') as f:
                        per_class = json.load(f)
                
                # Get threshold data if available
                thresholds = []
                threshold_path = os.path.join(model_path, 'threshold_analysis.json')
                if os.path.exists(threshold_path):
                    with open(threshold_path, 'r') as f:
                        thresholds = json.load(f)
                
                return {
                    'precision': summary.get('precision', 0),
                    'recall': summary.get('recall', 0),
                    'mAP50': summary.get('mAP50', 0),
                    'mAP50-95': summary.get('mAP50-95', 0),
                    'training_epochs': summary.get('final_epoch', 0),
                    'best_epoch': summary.get('best_epoch', 0),
                    'per_class': per_class,
                    'thresholds': thresholds,
                    'classes': summary.get('classes', 0)
                }
            
            # Fall back to results.csv if summary doesn't exist
            return ModelPerformanceService._parse_results_csv(os.path.join(model_path, 'results.csv'))
            
        except Exception as e:
            logging.error(f"Error getting performance metrics by ID: {str(e)}")
            return {
                'precision': 0,
                'recall': 0,
                'mAP50': 0,
                'mAP50-95': 0,
                'error': str(e),
                'per_class': {}
            }
    
    @staticmethod
    def _parse_results_csv(results_path):
        """Helper method to parse results.csv file."""
        if not os.path.exists(results_path):
            return {
                'precision': 0,
                'recall': 0,
                'mAP50': 0,
                'mAP50-95': 0,
                'per_class': {}
            }
            
        try:
            # Parse results.csv
            results = pd.read_csv(results_path)
            
            if len(results) == 0:
                return {
                    'precision': 0,
                    'recall': 0,
                    'mAP50': 0,
                    'mAP50-95': 0,
                    'per_class': {}
                }
            
            # Get the last row (final epoch)
            last_row = results.iloc[-1]
            
            # Find best epoch (highest mAP50)
            best_epoch = 0
            if 'mAP_0.5' in results.columns:
                best_idx = results['mAP_0.5'].idxmax()
                best_epoch = results.loc[best_idx, 'epoch']
            
            # Format performance metrics
            performance = {
                'precision': float(last_row.get('precision', 0)),
                'recall': float(last_row.get('recall', 0)),
                'mAP50': float(last_row.get('mAP_0.5', 0)),
                'mAP50-95': float(last_row.get('mAP_0.5:0.95', 0)),
                'training_epochs': int(last_row.get('epoch', 0)),
                'best_epoch': int(best_epoch),
                'per_class': {},
                'history': {
                    'epoch': results['epoch'].tolist() if 'epoch' in results.columns else [],
                    'precision': results['precision'].tolist() if 'precision' in results.columns else [],
                    'recall': results['recall'].tolist() if 'recall' in results.columns else [],
                    'mAP_0.5': results['mAP_0.5'].tolist() if 'mAP_0.5' in results.columns else [],
                    'mAP_0.5:0.95': results['mAP_0.5:0.95'].tolist() if 'mAP_0.5:0.95' in results.columns else [],
                    'box_loss': results['box_loss'].tolist() if 'box_loss' in results.columns else [],
                    'cls_loss': results['cls_loss'].tolist() if 'cls_loss' in results.columns else [],
                    'dfl_loss': results['dfl_loss'].tolist() if 'dfl_loss' in results.columns else []
                }
            }
            
            # Try to extract per-class metrics from column names
            if len(results.columns) > 10:
                # Look for per-class metrics in columns with patterns like 'precision_0', 'recall_0', etc.
                class_indices = set()
                for col in results.columns:
                    if col.startswith(('precision_', 'recall_', 'mAP50_')):
                        try:
                            idx = int(col.split('_')[-1])
                            class_indices.add(idx)
                        except ValueError:
                            continue
                
                # Get class names
                all_species = Species.query.all()
                class_names = {i: s.name for i, s in enumerate(all_species)}
                
                # Extract per-class metrics
                for idx in class_indices:
                    if idx in class_names:
                        class_name = class_names[idx]
                        performance['per_class'][class_name] = {
                            'precision': float(last_row.get(f'precision_{idx}', 0)),
                            'recall': float(last_row.get(f'recall_{idx}', 0)),
                            'map50': float(last_row.get(f'mAP50_{idx}', 0))
                        }
            
            return performance
        except Exception as e:
            logging.error(f"Error parsing results.csv: {str(e)}")
            return {
                'precision': 0,
                'recall': 0,
                'mAP50': 0,
                'mAP50-95': 0,
                'error': str(e),
                'per_class': {}
            }

    @staticmethod
    def get_confusion_matrix_by_id(model_id):
        """Get confusion matrix for a specific model by ID."""
        # Path to trained models
        models_dir = current_app.config.get('MODEL_FOLDER', 
                    os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(
                    os.path.abspath(__file__)))), 'models', 'trained'))
        
        # Check if model exists
        model_path = os.path.join(models_dir, model_id)
        if not os.path.exists(model_path) or not os.path.isdir(model_path):
            return ModelPerformanceService._create_placeholder_confusion_matrix()
        
        try:
            # Check for confusion_matrix.json
            confusion_path = os.path.join(model_path, 'confusion_matrix.json')
            if os.path.exists(confusion_path):
                with open(confusion_path, 'r') as f:
                    return json.load(f)
            
            # Check for alternative formats
            for path in [
                os.path.join(model_path, 'confusion_matrix.txt'),
                os.path.join(model_path, 'confusion_matrix.csv'),
                os.path.join(model_path, 'confusion_matrix.npy')
            ]:
                if os.path.exists(path):
                    return ModelPerformanceService._load_confusion_matrix(path)
            
            # If no confusion matrix found, create a placeholder
            return ModelPerformanceService._create_placeholder_confusion_matrix()
            
        except Exception as e:
            logging.error(f"Error getting confusion matrix by ID: {str(e)}")
            return ModelPerformanceService._create_placeholder_confusion_matrix(error=str(e))

    @staticmethod
    def _create_placeholder_confusion_matrix(error=None):
        """Create a placeholder confusion matrix for visualization."""
        all_species = Species.query.all()
        num_species = len(all_species)
        
        # Create a synthetic confusion matrix for visualization
        # with higher values on diagonal for visual clarity
        matrix = np.zeros((num_species, num_species))
        np.fill_diagonal(matrix, np.random.uniform(5, 20, num_species))
        
        # Add some off-diagonal elements (simulated misclassifications)
        for i in range(num_species):
            for j in range(num_species):
                if i != j:
                    # Higher chance of confusion between similar classes
                    matrix[i, j] = np.random.uniform(0, 5)
        
        class_names = [s.name for s in all_species]
        
        result = {
            'matrix': matrix.tolist(),
            'class_names': class_names,
            'synthetic': True  # Flag to indicate this is not real data
        }
        
        if error:
            result['error'] = error
            
        return result

    @staticmethod
    def _load_confusion_matrix(path):
        """Load confusion matrix from file."""
        ext = os.path.splitext(path)[1].lower()
        
        if ext in ['.txt', '.csv']:
            matrix = np.loadtxt(path)
        elif ext == '.npy':
            matrix = np.load(path)
        else:
            raise ValueError(f"Unsupported confusion matrix format: {ext}")
        
        # Get class names
        all_species = Species.query.all()
        class_names = [s.name for s in all_species]
        
        # Ensure matrix dimensions match number of classes
        if len(class_names) > 0:
            if matrix.shape[0] != len(class_names) or matrix.shape[1] != len(class_names):
                # Resize matrix if dimensions don't match
                new_matrix = np.zeros((len(class_names), len(class_names)))
                min_rows = min(matrix.shape[0], len(class_names))
                min_cols = min(matrix.shape[1], len(class_names))
                new_matrix[:min_rows, :min_cols] = matrix[:min_rows, :min_cols]
                matrix = new_matrix
        
        return {
            'matrix': matrix.tolist(),
            'class_names': class_names
        }

    @staticmethod
    def analyze_improvement_opportunities_by_id(model_id):
        """Analyze areas where a specific model could be improved."""
        # Path to trained models
        models_dir = current_app.config.get('MODEL_FOLDER', 
                    os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(
                    os.path.abspath(__file__)))), 'models', 'trained'))
        
        # Check if model exists
        model_path = os.path.join(models_dir, model_id)
        if not os.path.exists(model_path) or not os.path.isdir(model_path):
            return {
                'underrepresented_species': {},
                'problem_species': {},
                'improvement_suggestions': [
                    "Model not found. Please check the model ID."
                ]
            }
        
        try:
            # Look for existing improvement analysis
            improvement_path = os.path.join(model_path, 'improvement_analysis.json')
            if os.path.exists(improvement_path):
                with open(improvement_path, 'r') as f:
                    return json.load(f)
            
            # If not found, generate improvement analysis
            performance_metrics = ModelPerformanceService.get_performance_metrics_by_id(model_id)
            detection_stats = ModelPerformanceService.get_recent_detection_stats()
            
            # Count species representation in the dataset
            species_counts = {}
            for species in Species.query.all():
                count = Annotation.query.filter_by(species_id=species.id).count()
                species_counts[species.name] = count
                
            # Find underrepresented species (fewer than 50 examples)
            underrepresented = {}
            for species, count in species_counts.items():
                if count < 50 and count > 0:  # Only include species that exist but are underrepresented
                    underrepresented[species] = count
                    
            # Find species with high correction rates
            problem_species = {}
            for species, stats in detection_stats['species_corrections'].items():
                if stats['correction_rate'] > 25 and stats['total'] > 10:  # At least 25% correction rate and 10 examples
                    problem_species[species] = stats['correction_rate']
                    
            # Look for low-performing classes based on precision/recall
            low_performing_classes = {}
            if performance_metrics and 'per_class' in performance_metrics:
                for class_name, metrics in performance_metrics['per_class'].items():
                    precision = metrics.get('precision', 0)
                    recall = metrics.get('recall', 0)
                    map50 = metrics.get('map50', 0)
                    
                    # Flag classes with low precision or recall
                    if (precision < 0.5 or recall < 0.3) and class_name in species_counts and species_counts[class_name] > 10:
                        low_performing_classes[class_name] = {
                            'precision': precision,
                            'recall': recall,
                            'map50': map50,
                            'count': species_counts.get(class_name, 0)
                        }
            
            # Generate improvement suggestions
            suggestions = []
            
            if performance_metrics.get('mAP50', 0) < 0.7:
                suggestions.append("The model's overall accuracy (mAP50) is below 70%. Consider more training data or model tuning.")
                
            if underrepresented:
                top_underrepresented = sorted(underrepresented.items(), key=lambda x: x[1])[:5]
                species_list = ', '.join([f"{s} ({c} examples)" for s, c in top_underrepresented])
                suggestions.append(f"Collect more training data for underrepresented species: {species_list}")
                
            if problem_species:
                top_problems = sorted(problem_species.items(), key=lambda x: x[1], reverse=True)[:5]
                species_list = ', '.join([f"{s} ({c:.1f}% correction rate)" for s, c in top_problems])
                suggestions.append(f"Review and improve annotations for frequently corrected species: {species_list}")
            
            if low_performing_classes:
                low_perf_list = ', '.join([f"{s} (P:{c['precision']:.2f}, R:{c['recall']:.2f})" 
                                        for s, c in list(low_performing_classes.items())[:3]])
                suggestions.append(f"Focus on improving detection for low-performing classes: {low_perf_list}")
                
            # Check for precision/recall imbalance
            if performance_metrics.get('precision', 0) > performance_metrics.get('recall', 0) + 0.2:
                suggestions.append("Model has high precision but lower recall. Consider using a lower confidence threshold for inference or augmenting training data.")
            elif performance_metrics.get('recall', 0) > performance_metrics.get('precision', 0) + 0.2:
                suggestions.append("Model has high recall but lower precision. Consider training with higher box loss weight or using a higher confidence threshold.")
                
            # Additional generic suggestions
            suggestions.append("Consider data augmentation techniques to improve model robustness.")
            suggestions.append("Evaluate model performance in different lighting conditions and environments.")
            
            # Get taxonomic group performance if available
            taxonomic_group_performance = {}
            group_path = os.path.join(model_path, 'taxonomic_performance.json')
            if os.path.exists(group_path):
                with open(group_path, 'r') as f:
                    taxonomic_group_performance = json.load(f)
            
            result = {
                'underrepresented_species': underrepresented,
                'problem_species': problem_species,
                'low_performing_classes': low_performing_classes,
                'improvement_suggestions': suggestions,
                'taxonomic_group_performance': taxonomic_group_performance
            }
            
            return result
            
        except Exception as e:
            logging.error(f"Error analyzing improvement opportunities by ID: {str(e)}")
            return {
                'underrepresented_species': {},
                'problem_species': {},
                'improvement_suggestions': [
                    "Error analyzing improvement opportunities. Check logs for details.",
                    "Consider data augmentation techniques to improve model robustness.",
                    "Evaluate model performance in different lighting conditions and environments."
                ],
                'error': str(e)
            }

    @staticmethod
    def get_training_history_by_id(model_id):
        """Get the training history for a specific model."""
        # Path to trained models
        models_dir = current_app.config.get('MODEL_FOLDER', 
                    os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(
                    os.path.abspath(__file__)))), 'models', 'trained'))
        
        # Check if model exists
        model_path = os.path.join(models_dir, model_id)
        if not os.path.exists(model_path) or not os.path.isdir(model_path):
            return None
            
        try:
            # Check for training_history.json
            history_path = os.path.join(model_path, 'training_history.json')
            if os.path.exists(history_path):
                with open(history_path, 'r') as f:
                    return json.load(f)
            
            # Fall back to results.csv
            results_path = os.path.join(model_path, 'results.csv')
            if os.path.exists(results_path):
                results_df = pd.read_csv(results_path)
                
                # Extract key columns for history
                history_cols = ['epoch', 'box_loss', 'cls_loss', 'dfl_loss', 'precision', 'recall', 'mAP_0.5', 'mAP_0.5:0.95']
                available_cols = [col for col in history_cols if col in results_df.columns]
                
                history = {}
                for col in available_cols:
                    history[col] = results_df[col].tolist()
                
                return history
                
            return None
            
        except Exception as e:
            logging.error(f"Error getting training history by ID: {str(e)}")
            return None