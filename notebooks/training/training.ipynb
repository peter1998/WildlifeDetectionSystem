{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "41c06ffb-c158-4912-94cd-2fa9e711ebfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version: 3.12.3\n",
      "PyTorch version: 2.6.0+cu124\n",
      "CUDA available: True\n",
      "GPU device: NVIDIA GeForce RTX 4050 Laptop GPU\n",
      "CUDA version: 12.4\n",
      "GPU memory: 6.18 GB total\n",
      "✅ numpy is installed (version: 2.1.1)\n",
      "✅ matplotlib is installed (version: 3.10.1)\n",
      "✅ pandas is installed (version: 2.2.3)\n",
      "✅ ultralytics is installed (version: 8.3.106)\n",
      "✅ opencv-python is installed (version: 4.11.0)\n",
      "✅ seaborn is installed (version: 0.13.2)\n"
     ]
    }
   ],
   "source": [
    "# Wildlife Detection System - Model Analysis Notebook\n",
    "# \n",
    "# This notebook provides comprehensive analysis of the trained YOLOv8 model for \n",
    "# wildlife detection, including:\n",
    "# - Model performance metrics across different classes and taxonomic groups\n",
    "# - Confusion matrix analysis to identify misclassifications\n",
    "# - Failure case analysis to understand model shortcomings\n",
    "# - Image analysis to understand environmental factors affecting detection\n",
    "# - Suggestions for model improvement based on the analysis\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import cv2\n",
    "import torch\n",
    "import json\n",
    "from PIL import Image\n",
    "from datetime import datetime\n",
    "from ultralytics import YOLO\n",
    "from IPython.display import display, Image as IPImage\n",
    "\n",
    "# Set plot style and figure size for better visualization\n",
    "plt.style.use('ggplot')\n",
    "plt.rcParams['figure.figsize'] = (14, 8)\n",
    "sns.set_palette('viridis')\n",
    "\n",
    "# System paths and configuration\n",
    "ROOT_DIR = Path('/home/peter/Desktop/TU PHD/WildlifeDetectionSystem')\n",
    "MODELS_DIR = ROOT_DIR / 'models' / 'trained'\n",
    "DATA_DIR = ROOT_DIR / 'data'\n",
    "EXPORT_DIR = DATA_DIR / 'export'\n",
    "OUTPUT_DIR = ROOT_DIR / 'output'\n",
    "REPORTS_DIR = ROOT_DIR / 'reports'\n",
    "\n",
    "# Create output directories if they don't exist\n",
    "os.makedirs(OUTPUT_DIR / 'model_analysis', exist_ok=True)\n",
    "ANALYSIS_DIR = REPORTS_DIR / f'model_analysis_{datetime.now().strftime(\"%Y%m%d_%H%M\")}'\n",
    "os.makedirs(ANALYSIS_DIR, exist_ok=True)\n",
    "\n",
    "# Environment check\n",
    "print(f\"Python version: {sys.version.split()[0]}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "    print(f\"GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB total\")\n",
    "\n",
    "# Check imported libraries\n",
    "for lib in ['numpy', 'matplotlib', 'pandas', 'ultralytics', 'opencv-python', 'seaborn']:\n",
    "    try:\n",
    "        if lib == 'opencv-python':\n",
    "            import cv2\n",
    "            version = cv2.__version__\n",
    "        elif lib == 'matplotlib':\n",
    "            import matplotlib\n",
    "            version = matplotlib.__version__\n",
    "        elif lib == 'numpy':\n",
    "            import numpy\n",
    "            version = numpy.__version__\n",
    "        elif lib == 'pandas':\n",
    "            import pandas\n",
    "            version = pandas.__version__\n",
    "        elif lib == 'ultralytics':\n",
    "            import ultralytics\n",
    "            version = ultralytics.__version__\n",
    "        elif lib == 'seaborn':\n",
    "            import seaborn\n",
    "            version = seaborn.__version__\n",
    "        print(f\"✅ {lib} is installed (version: {version})\")\n",
    "    except ImportError:\n",
    "        print(f\"❌ {lib} is not installed\")\n",
    "\n",
    "# Utility functions for model and data loading\n",
    "def find_latest_model():\n",
    "    \"\"\"Find the most recently trained model in the models directory\"\"\"\n",
    "    models = list(MODELS_DIR.glob('wildlife_detector_*'))\n",
    "    if not models:\n",
    "        return None\n",
    "    \n",
    "    # Sort by creation time (newest first)\n",
    "    latest_model = max(models, key=lambda p: p.stat().st_mtime)\n",
    "    \n",
    "    # Check for best.pt weights\n",
    "    best_weights = latest_model / 'weights' / 'best.pt'\n",
    "    if best_weights.exists():\n",
    "        return best_weights\n",
    "    \n",
    "    # If best.pt doesn't exist, try last.pt\n",
    "    last_weights = latest_model / 'weights' / 'last.pt'\n",
    "    if last_weights.exists():\n",
    "        return last_weights\n",
    "    \n",
    "    return None\n",
    "\n",
    "def find_data_yaml():\n",
    "    \"\"\"Find the most recent data.yaml file in the export directory\"\"\"\n",
    "    yaml_files = list(EXPORT_DIR.glob('**/data.yaml'))\n",
    "    if not yaml_files:\n",
    "        return None\n",
    "    \n",
    "    # Sort by creation time (newest first)\n",
    "    return max(yaml_files, key=lambda p: p.stat().st_mtime)\n",
    "\n",
    "def load_class_names(yaml_path):\n",
    "    \"\"\"Load class names from data.yaml file\"\"\"\n",
    "    import yaml\n",
    "    with open(yaml_path, 'r') as f:\n",
    "        data = yaml.safe_load(f)\n",
    "    return data.get('names', [])\n",
    "\n",
    "def load_specific_model(model_dir='wildlife_detector_20250503_1345'):\n",
    "    \"\"\"Load a specific model by directory name\"\"\"\n",
    "    model_path = MODELS_DIR / model_dir / 'weights' / 'best.pt'\n",
    "    if not model_path.exists():\n",
    "        model_path = MODELS_DIR / model_dir / 'weights' / 'last.pt'\n",
    "    \n",
    "    if not model_path.exists():\n",
    "        print(f\"⚠️ Model not found: {model_path}\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"✅ Loading model: {model_path}\")\n",
    "    return model_path\n",
    "\n",
    "# Define taxonomic groups for wildlife classification\n",
    "def get_taxonomic_groups(class_names):\n",
    "    \"\"\"Define taxonomic groups for better analysis\"\"\"\n",
    "    taxonomic_groups = {\n",
    "        'Deer': [i for i, name in enumerate(class_names) \n",
    "                if any(deer in name.lower() for deer in ['deer', 'roe', 'fallow', 'red deer'])],\n",
    "        'Carnivores': [i for i, name in enumerate(class_names) \n",
    "                      if any(carnivore in name.lower() for carnivore in \n",
    "                            ['fox', 'wolf', 'jackal', 'bear', 'badger', 'weasel', 'stoat', \n",
    "                             'polecat', 'marten', 'otter', 'wildcat'])],\n",
    "        'Small_Mammals': [i for i, name in enumerate(class_names) \n",
    "                         if any(small in name.lower() for small in \n",
    "                               ['rabbit', 'hare', 'squirrel', 'dormouse', 'hedgehog'])],\n",
    "        'Birds': [i for i, name in enumerate(class_names) \n",
    "                 if any(bird in name.lower() for bird in \n",
    "                       ['blackbird', 'nightingale', 'pheasant', 'woodpecker'])],\n",
    "        'Other': [i for i, name in enumerate(class_names) \n",
    "                 if any(other in name.lower() for other in \n",
    "                       ['wild boar', 'chamois', 'turtle', 'human', 'background', 'dog'])]\n",
    "    }\n",
    "    return taxonomic_groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5f5c53c4-1d72-40ba-83ee-0685543fbdfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loading model: /home/peter/Desktop/TU PHD/WildlifeDetectionSystem/models/trained/wildlife_detector_20250503_1345/weights/best.pt\n",
      "Loaded 30 classes from /home/peter/Desktop/TU PHD/WildlifeDetectionSystem/data/export/yolo_default_20250429_085945/data.yaml\n",
      "Classes: Red Deer, Male Roe Deer, Female Roe Deer, Fallow Deer, Wild Boar, Chamois, Fox, Wolf, Jackal, Brown Bear...\n",
      "Deer: 4 classes - Red Deer, Male Roe Deer, Female Roe Deer, Fallow Deer\n",
      "Carnivores: 11 classes - Fox, Wolf, Jackal, Brown Bear, Badger...\n",
      "Small_Mammals: 5 classes - Rabbit, Hare, Squirrel, Dormouse, Hedgehog\n",
      "Birds: 4 classes - Blackbird, Nightingale, Pheasant, woodpecker\n",
      "Other: 6 classes - Wild Boar, Chamois, Turtle, Human, Background...\n",
      "Model loaded successfully: best.pt\n",
      "Model type: best\n",
      "Model summary: 129 layers, 3,016,698 parameters, 0 gradients, 8.2 GFLOPs\n",
      "Model info not available\n",
      "Model analyzer initialized successfully!\n"
     ]
    }
   ],
   "source": [
    "# Load and initialize the model for analysis\n",
    "\n",
    "# Find the model path\n",
    "model_path = load_specific_model()  # Use the wildlife_detector_20250503_1345 model by default\n",
    "if model_path is None:\n",
    "    model_path = find_latest_model()\n",
    "    if model_path is None:\n",
    "        raise FileNotFoundError(\"No trained model found in the models directory!\")\n",
    "\n",
    "# Find the data configuration\n",
    "data_yaml = find_data_yaml()\n",
    "if data_yaml is None:\n",
    "    raise FileNotFoundError(\"No data.yaml file found in the export directory!\")\n",
    "\n",
    "# Load class names\n",
    "class_names = load_class_names(data_yaml)\n",
    "print(f\"Loaded {len(class_names)} classes from {data_yaml}\")\n",
    "print(f\"Classes: {', '.join(class_names[:10])}{'...' if len(class_names) > 10 else ''}\")\n",
    "\n",
    "# Define taxonomic groups\n",
    "taxonomic_groups = get_taxonomic_groups(class_names)\n",
    "for group, indices in taxonomic_groups.items():\n",
    "    class_list = [class_names[i] for i in indices if i < len(class_names)]\n",
    "    print(f\"{group}: {len(class_list)} classes - {', '.join(class_list[:5])}{'...' if len(class_list) > 5 else ''}\")\n",
    "\n",
    "# Load the model\n",
    "try:\n",
    "    model = YOLO(model_path)\n",
    "    print(f\"Model loaded successfully: {model_path.name}\")\n",
    "    \n",
    "    # Print model info\n",
    "    model_type = model_path.stem  # best or last\n",
    "    print(f\"Model type: {model_type}\")\n",
    "    \n",
    "    # Get model info\n",
    "    model_info = model.info()\n",
    "    if isinstance(model_info, dict):\n",
    "        print(f\"Input image size: {model_info.get('imgsz', 'unknown')}x{model_info.get('imgsz', 'unknown')}\")\n",
    "        print(f\"Model contains {model_info.get('nc', 'unknown')} classes\")\n",
    "    else:\n",
    "        print(\"Model info not available\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error loading model: {e}\")\n",
    "    raise\n",
    "\n",
    "# Create a model analyzer class to organize our analysis functions\n",
    "class ModelAnalyzer:\n",
    "    def __init__(self, model, class_names, taxonomic_groups):\n",
    "        self.model = model\n",
    "        self.class_names = class_names\n",
    "        self.taxonomic_groups = taxonomic_groups\n",
    "        self.results_df = None\n",
    "        self.confusion_matrix = None\n",
    "        \n",
    "    def get_group_for_class(self, class_idx):\n",
    "        \"\"\"Return the taxonomic group for a given class index\"\"\"\n",
    "        for group, indices in self.taxonomic_groups.items():\n",
    "            if class_idx in indices:\n",
    "                return group\n",
    "        return \"Unknown\"\n",
    "    \n",
    "    def evaluate_on_dataset(self, data_path):\n",
    "        \"\"\"Run evaluation on a dataset and return metrics\"\"\"\n",
    "        results = self.model.val(data=data_path)\n",
    "        return results\n",
    "    \n",
    "    def predict_image(self, image_path, conf=0.25):\n",
    "        \"\"\"Run prediction on a single image\"\"\"\n",
    "        results = self.model.predict(image_path, conf=conf)\n",
    "        return results\n",
    "    \n",
    "    def collect_results_from_folder(self, folder_path, limit=20, conf=0.25):\n",
    "        \"\"\"Run predictions on images in a folder and collect results\"\"\"\n",
    "        import glob\n",
    "        \n",
    "        # Find image files\n",
    "        image_files = []\n",
    "        for ext in ['jpg', 'jpeg', 'png', 'JPG', 'JPEG', 'PNG']:\n",
    "            image_files.extend(glob.glob(os.path.join(folder_path, f'*.{ext}')))\n",
    "        \n",
    "        # Limit the number of images\n",
    "        if limit > 0 and len(image_files) > limit:\n",
    "            image_files = image_files[:limit]\n",
    "        \n",
    "        print(f\"Processing {len(image_files)} images...\")\n",
    "        \n",
    "        # Process each image\n",
    "        results = []\n",
    "        for image_file in image_files:\n",
    "            prediction = self.model.predict(image_file, conf=conf)\n",
    "            if len(prediction) > 0:\n",
    "                for pred in prediction:\n",
    "                    if len(pred.boxes.cls) > 0:\n",
    "                        results.append({\n",
    "                            'image_path': image_file,\n",
    "                            'prediction': pred\n",
    "                        })\n",
    "        \n",
    "        print(f\"Found detections in {len(results)} images\")\n",
    "        return results\n",
    "    \n",
    "    def visualize_detection(self, image_path, conf=0.25):\n",
    "        \"\"\"Visualize detection results on an image\"\"\"\n",
    "        # Predict on image\n",
    "        results = self.predict_image(image_path, conf)\n",
    "        \n",
    "        # Load the image\n",
    "        img = cv2.imread(image_path)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Draw bounding boxes and labels\n",
    "        for result in results:\n",
    "            boxes = result.boxes.xyxy.cpu().numpy().astype(int)\n",
    "            classes = result.boxes.cls.cpu().numpy().astype(int)\n",
    "            confidences = result.boxes.conf.cpu().numpy()\n",
    "            \n",
    "            for i, (box, cls, conf) in enumerate(zip(boxes, classes, confidences)):\n",
    "                x1, y1, x2, y2 = box\n",
    "                class_name = self.class_names[cls] if cls < len(self.class_names) else \"Unknown\"\n",
    "                label = f\"{class_name} ({conf:.2f})\"\n",
    "                \n",
    "                # Get color based on taxonomic group\n",
    "                group = self.get_group_for_class(cls)\n",
    "                color_map = {\n",
    "                    'Deer': (0, 255, 0),      # Green\n",
    "                    'Carnivores': (0, 0, 255), # Blue\n",
    "                    'Small_Mammals': (255, 165, 0),  # Orange\n",
    "                    'Birds': (128, 0, 128),   # Purple\n",
    "                    'Other': (255, 0, 0)      # Red\n",
    "                }\n",
    "                color = color_map.get(group, (255, 255, 255))\n",
    "                \n",
    "                # Draw bounding box\n",
    "                cv2.rectangle(img, (x1, y1), (x2, y2), color, 2)\n",
    "                \n",
    "                # Draw label\n",
    "                (w, h), _ = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.6, 1)\n",
    "                cv2.rectangle(img, (x1, y1-25), (x1+w, y1), color, -1)\n",
    "                cv2.putText(img, label, (x1, y1-5), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)\n",
    "        \n",
    "        # Display the image\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        plt.imshow(img)\n",
    "        plt.axis('off')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Return the results\n",
    "        return results\n",
    "\n",
    "# Initialize the model analyzer\n",
    "analyzer = ModelAnalyzer(model, class_names, taxonomic_groups)\n",
    "print(\"Model analyzer initialized successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "caf4bfb0-0f7e-4d83-bb6d-1798956d663b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model evaluation on validation dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/peter/Desktop/TU PHD/WildlifeDetectionSystem/data/export/yolo_default_20250429_085945/labels/val.cache... 89 images, 0 backgrounds, 3 corrupt: 100%|██████████| 89/89 [00:00<?, ?it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 6/6 [00:00<00:00,  6.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Overall Performance Metrics:\n",
      "- mAP50: 0.6236\n",
      "- mAP50-95: 0.4129\n",
      "- Precision: 0.7877\n",
      "- Recall: 0.5138\n",
      "\n",
      "Analyzing class-specific performance...\n",
      "Available attributes and methods in results.box:\n",
      "- all_ap\n",
      "- ap\n",
      "- ap50\n",
      "- ap_class_index\n",
      "- class_result\n",
      "- curves\n",
      "- curves_results\n",
      "- f1\n",
      "- f1_curve\n",
      "- fitness\n",
      "- map\n",
      "- map50\n",
      "- map75\n",
      "- maps\n",
      "- mean_results\n",
      "- mp\n",
      "- mr\n",
      "- nc\n",
      "- p\n",
      "- p_curve\n",
      "- prec_values\n",
      "- px\n",
      "- r\n",
      "- r_curve\n",
      "- update\n",
      "Could not get per-class AP values from methods, using basic values\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1600x1000 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Analyzing performance by taxonomic group...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1400x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Analyzing impact of confidence thresholds...\n",
      "Evaluating with confidence threshold: 0.25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/peter/Desktop/TU PHD/WildlifeDetectionSystem/data/export/yolo_default_20250429_085945/labels/val.cache... 89 images, 0 backgrounds, 3 corrupt: 100%|██████████| 89/89 [00:00<?, ?it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 6/6 [00:01<00:00,  5.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating with confidence threshold: 0.50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/peter/Desktop/TU PHD/WildlifeDetectionSystem/data/export/yolo_default_20250429_085945/labels/val.cache... 89 images, 0 backgrounds, 3 corrupt: 100%|██████████| 89/89 [00:00<?, ?it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 6/6 [00:00<00:00,  6.15it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Visualizing sample predictions...\n",
      "Found 50 image files, selecting 2 for sample visualization\n",
      "Image: 1488_15_03_24_Моллова_курия_IMAG0345.JPG\n",
      "Found 1 detections with confidence > 0.25\n",
      "  Detection 1: Male Roe Deer (confidence: 0.93)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Image: 0516_27_01_2024_100BMCIM_IMAG0017.JPG\n",
      "Found 1 detections with confidence > 0.25\n",
      "  Detection 1: Female Roe Deer (confidence: 0.80)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Generating model analysis report...\n",
      "Analysis complete! Report saved to: /home/peter/Desktop/TU PHD/WildlifeDetectionSystem/reports/model_analysis_20250504_1104/model_analysis_report.md\n"
     ]
    }
   ],
   "source": [
    "# Analyze model performance and visualize results\n",
    "\n",
    "# Run evaluation on validation dataset with reduced logging\n",
    "import logging\n",
    "import warnings\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# Set up logging to suppress warnings\n",
    "logging.getLogger('ultralytics').setLevel(logging.ERROR)\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "os.environ['ULTRALYTICS_QUIET'] = '1'  # Reduce YOLO verbose output\n",
    "\n",
    "print(\"Running model evaluation on validation dataset...\")\n",
    "eval_results = analyzer.evaluate_on_dataset(data_yaml)\n",
    "\n",
    "# Extract performance metrics\n",
    "if hasattr(eval_results, 'box') and hasattr(eval_results.box, 'map'):\n",
    "    map50 = float(eval_results.box.map50)\n",
    "    map50_95 = float(eval_results.box.map)\n",
    "    \n",
    "    # Handle potential numpy arrays by converting to float\n",
    "    try:\n",
    "        precision = float(eval_results.box.p)\n",
    "        recall = float(eval_results.box.r)\n",
    "    except (TypeError, ValueError):\n",
    "        # If values are arrays, take the mean\n",
    "        precision = float(eval_results.box.p.mean()) if hasattr(eval_results.box.p, 'mean') else 0.0\n",
    "        recall = float(eval_results.box.r.mean()) if hasattr(eval_results.box.r, 'mean') else 0.0\n",
    "    \n",
    "    print(f\"\\nOverall Performance Metrics:\")\n",
    "    print(f\"- mAP50: {map50:.4f}\")\n",
    "    print(f\"- mAP50-95: {map50_95:.4f}\")\n",
    "    print(f\"- Precision: {precision:.4f}\")\n",
    "    print(f\"- Recall: {recall:.4f}\")\n",
    "else:\n",
    "    print(\"Could not extract performance metrics from evaluation results\")\n",
    "\n",
    "# Create performance visualization for classes\n",
    "def visualize_class_performance(results):\n",
    "    \"\"\"Visualize performance metrics by class\"\"\"\n",
    "    if not hasattr(results, 'names') or not hasattr(results, 'box'):\n",
    "        print(\"Results don't contain class-specific metrics\")\n",
    "        return\n",
    "    \n",
    "    # Extract class metrics - updated to use class_result method instead of direct attribute access\n",
    "    metrics = []\n",
    "    \n",
    "    # Check what attributes are available\n",
    "    print(\"Available attributes and methods in results.box:\")\n",
    "    for attr in dir(results.box):\n",
    "        if not attr.startswith('_'):\n",
    "            print(f\"- {attr}\")\n",
    "    \n",
    "    # Try to access per-class metrics using the appropriate methods\n",
    "    try:\n",
    "        # New approach using available methods\n",
    "        if hasattr(results.box, 'nc') and results.box.nc > 0:\n",
    "            # Get number of classes\n",
    "            num_classes = results.box.nc\n",
    "            \n",
    "            # Use ap50 and ap methods to get per-class AP values\n",
    "            try:\n",
    "                ap50_per_class = results.box.ap50()\n",
    "                ap_per_class = results.box.ap()\n",
    "            except (AttributeError, TypeError):\n",
    "                # Fallbacks if methods don't exist\n",
    "                print(\"Could not get per-class AP values from methods, using basic values\")\n",
    "                ap50_per_class = [map50] * num_classes\n",
    "                ap_per_class = [map50_95] * num_classes\n",
    "            \n",
    "            # Get class precisions and recalls if available\n",
    "            if hasattr(results.box, 'p') and isinstance(results.box.p, (list, np.ndarray)) and len(results.box.p) >= num_classes:\n",
    "                precisions = results.box.p\n",
    "                recalls = results.box.r\n",
    "            else:\n",
    "                # Fallbacks\n",
    "                precisions = [precision] * num_classes\n",
    "                recalls = [recall] * num_classes\n",
    "            \n",
    "            # Get class indices if available\n",
    "            if hasattr(results.box, 'ap_class_index'):\n",
    "                class_indices = results.box.ap_class_index\n",
    "            else:\n",
    "                class_indices = list(range(num_classes))\n",
    "            \n",
    "            # Build metrics for each class\n",
    "            for i, class_idx in enumerate(class_indices):\n",
    "                if i < len(ap50_per_class) and class_idx < len(results.names):\n",
    "                    class_name = results.names[class_idx]\n",
    "                    # Convert to float to avoid numpy array issues\n",
    "                    metrics.append({\n",
    "                        'Class': class_name,\n",
    "                        'mAP50': float(ap50_per_class[i]),\n",
    "                        'mAP50-95': float(ap_per_class[i]),\n",
    "                        'Precision': float(precisions[class_idx] if class_idx < len(precisions) else 0),\n",
    "                        'Recall': float(recalls[class_idx] if class_idx < len(recalls) else 0),\n",
    "                        'Count': 1  # Default count if not available\n",
    "                    })\n",
    "        else:\n",
    "            print(\"No class metrics available in results\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting class metrics: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(metrics)\n",
    "    if len(df) == 0:\n",
    "        print(\"No class metrics available\")\n",
    "        return\n",
    "    \n",
    "    # Sort by mAP50\n",
    "    df = df.sort_values('mAP50', ascending=False)\n",
    "    \n",
    "    # Plot class performance\n",
    "    plt.figure(figsize=(16, 10))\n",
    "    \n",
    "    # Create class performance chart\n",
    "    ax = plt.subplot(1, 1, 1)  # Changed to single plot since we don't have counts\n",
    "    x = range(len(df))\n",
    "    \n",
    "    # Plot metrics\n",
    "    ax.bar(x, df['mAP50'], alpha=0.6, color='blue', label='mAP50')\n",
    "    ax.bar([i + 0.2 for i in x], df['Precision'], alpha=0.6, color='green', label='Precision')\n",
    "    ax.bar([i + 0.4 for i in x], df['Recall'], alpha=0.6, color='red', label='Recall')\n",
    "    \n",
    "    # Set labels and title\n",
    "    ax.set_xticks([i + 0.2 for i in x])\n",
    "    ax.set_xticklabels(df['Class'], rotation=45, ha='right')\n",
    "    ax.set_ylabel('Metric Value')\n",
    "    ax.set_ylim(0, 1.1)\n",
    "    ax.set_title('Model Performance by Class')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(str(ANALYSIS_DIR / 'class_performance.png'), dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # Return the dataframe for further analysis\n",
    "    return df\n",
    "\n",
    "# Create performance visualization by taxonomic group\n",
    "def visualize_group_performance(class_df, taxonomic_groups, class_names):\n",
    "    \"\"\"Visualize performance metrics by taxonomic group\"\"\"\n",
    "    if class_df is None or len(class_df) == 0:\n",
    "        print(\"No class metrics available\")\n",
    "        return\n",
    "    \n",
    "    # Map classes to groups\n",
    "    class_to_group = {}\n",
    "    for group, indices in taxonomic_groups.items():\n",
    "        for idx in indices:\n",
    "            if idx < len(class_names):\n",
    "                class_to_group[class_names[idx]] = group\n",
    "    \n",
    "    # Add group to dataframe\n",
    "    class_df['Group'] = class_df['Class'].apply(lambda x: class_to_group.get(x, 'Unknown'))\n",
    "    \n",
    "    # Aggregate metrics by group\n",
    "    group_df = class_df.groupby('Group').agg({\n",
    "        'mAP50': 'mean',\n",
    "        'mAP50-95': 'mean',\n",
    "        'Precision': 'mean',\n",
    "        'Recall': 'mean'\n",
    "    }).reset_index()\n",
    "    \n",
    "    # Plot group performance\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    \n",
    "    # Create group performance chart\n",
    "    ax = plt.subplot(1, 1, 1)\n",
    "    x = range(len(group_df))\n",
    "    \n",
    "    # Plot metrics\n",
    "    ax.bar(x, group_df['mAP50'], alpha=0.6, color='blue', label='mAP50')\n",
    "    ax.bar([i + 0.2 for i in x], group_df['Precision'], alpha=0.6, color='green', label='Precision')\n",
    "    ax.bar([i + 0.4 for i in x], group_df['Recall'], alpha=0.6, color='red', label='Recall')\n",
    "    \n",
    "    # Set labels and title\n",
    "    ax.set_xticks([i + 0.2 for i in x])\n",
    "    ax.set_xticklabels(group_df['Group'], rotation=0)\n",
    "    ax.set_ylabel('Metric Value')\n",
    "    ax.set_ylim(0, 1.1)\n",
    "    ax.set_title('Model Performance by Taxonomic Group')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(str(ANALYSIS_DIR / 'group_performance.png'), dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # Return the dataframe for further analysis\n",
    "    return group_df\n",
    "\n",
    "# Analyze results at different confidence thresholds\n",
    "def analyze_confidence_thresholds(model, data_path, thresholds=[0.25, 0.5]):\n",
    "    \"\"\"Analyze model performance at different confidence thresholds\"\"\"\n",
    "    results = []\n",
    "    for conf in thresholds:\n",
    "        print(f\"Evaluating with confidence threshold: {conf:.2f}\")\n",
    "        result = model.val(data=data_path, conf=conf, verbose=False)  # Reduce output verbosity\n",
    "        \n",
    "        # Convert all values to float to avoid numpy array issues\n",
    "        results.append({\n",
    "            'Threshold': conf,\n",
    "            'mAP50': float(result.box.map50),\n",
    "            'Precision': float(result.box.p) if not hasattr(result.box.p, 'mean') else float(result.box.p.mean()),\n",
    "            'Recall': float(result.box.r) if not hasattr(result.box.r, 'mean') else float(result.box.r.mean())\n",
    "        })\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(results)\n",
    "    \n",
    "    # Plot results\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.plot(df['Threshold'], df['mAP50'], 'o-', label='mAP50', linewidth=2, markersize=10)\n",
    "    plt.plot(df['Threshold'], df['Precision'], 's-', label='Precision', linewidth=2, markersize=10)\n",
    "    plt.plot(df['Threshold'], df['Recall'], '^-', label='Recall', linewidth=2, markersize=10)\n",
    "    \n",
    "    plt.xlabel('Confidence Threshold')\n",
    "    plt.ylabel('Metric Value')\n",
    "    plt.title('Model Performance vs. Confidence Threshold')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.legend()\n",
    "    plt.savefig(str(ANALYSIS_DIR / 'confidence_threshold_analysis.png'), dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # Return the dataframe for further analysis\n",
    "    return df\n",
    "\n",
    "# Visualize sample predictions (limited to just a few images)\n",
    "def visualize_sample_predictions(model, image_folder, num_samples=2, conf=0.25):\n",
    "    \"\"\"Visualize predictions on a few sample images\"\"\"\n",
    "    import random\n",
    "    import glob\n",
    "    \n",
    "    # Find image files\n",
    "    image_files = []\n",
    "    for ext in ['jpg', 'jpeg', 'png', 'JPG', 'JPEG', 'PNG']:\n",
    "        pattern = os.path.join(image_folder, f'*.{ext}')\n",
    "        found_files = glob.glob(pattern)\n",
    "        if found_files:\n",
    "            image_files.extend(found_files[:50])  # Limit to 50 files per extension\n",
    "    \n",
    "    if not image_files:\n",
    "        print(f\"No image files found in {image_folder}\")\n",
    "        return\n",
    "        \n",
    "    print(f\"Found {len(image_files)} image files, selecting {num_samples} for sample visualization\")\n",
    "    \n",
    "    # Randomly select samples\n",
    "    if len(image_files) > num_samples:\n",
    "        samples = random.sample(image_files, num_samples)\n",
    "    else:\n",
    "        samples = image_files\n",
    "    \n",
    "    # Process each sample\n",
    "    for image_file in samples:\n",
    "        try:\n",
    "            print(f\"Image: {os.path.basename(image_file)}\")\n",
    "            results = model.predict(image_file, conf=conf, verbose=False)  # Reduce verbosity\n",
    "            \n",
    "            if len(results) > 0 and len(results[0].boxes) > 0:\n",
    "                # Get results\n",
    "                boxes = results[0].boxes\n",
    "                print(f\"Found {len(boxes)} detections with confidence > {conf}\")\n",
    "                \n",
    "                # Print detection information (limit to first 3 to avoid cluttering output)\n",
    "                max_detections = min(3, len(boxes))\n",
    "                for i in range(max_detections):\n",
    "                    box = boxes[i]\n",
    "                    cls = int(box.cls.item())\n",
    "                    conf_val = box.conf.item()\n",
    "                    cls_name = class_names[cls] if cls < len(class_names) else \"Unknown\"\n",
    "                    print(f\"  Detection {i+1}: {cls_name} (confidence: {conf_val:.2f})\")\n",
    "                \n",
    "                if len(boxes) > max_detections:\n",
    "                    print(f\"  ... and {len(boxes) - max_detections} more detections\")\n",
    "            else:\n",
    "                print(\"  No detections found\")\n",
    "            \n",
    "            # Show image with predictions\n",
    "            img = cv2.imread(image_file)\n",
    "            if img is None:\n",
    "                print(f\"  Warning: Could not read image file {image_file}\")\n",
    "                continue\n",
    "                \n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "            \n",
    "            plt.figure(figsize=(10, 8))\n",
    "            \n",
    "            # If results exist, plot them on the image\n",
    "            if len(results) > 0 and len(results[0].boxes) > 0:\n",
    "                # Plot image with results\n",
    "                result_plotted = results[0].plot(line_width=2, font_size=1.5)\n",
    "                plt.imshow(result_plotted)\n",
    "            else:\n",
    "                # Just show the image\n",
    "                plt.imshow(img)\n",
    "            \n",
    "            plt.title(f\"Image: {os.path.basename(image_file)}\")\n",
    "            plt.axis('off')\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing image {image_file}: {e}\")\n",
    "        \n",
    "        print(\"\\n\" + \"-\"*50 + \"\\n\")\n",
    "\n",
    "# Run analysis with more diagnostic output\n",
    "try:\n",
    "    # Run class performance analysis\n",
    "    print(\"\\nAnalyzing class-specific performance...\")\n",
    "    class_df = visualize_class_performance(eval_results)\n",
    "\n",
    "    # If class_df was successfully created, continue with group analysis\n",
    "    if class_df is not None and len(class_df) > 0:\n",
    "        print(\"\\nAnalyzing performance by taxonomic group...\")\n",
    "        group_df = visualize_group_performance(class_df, taxonomic_groups, class_names)\n",
    "    else:\n",
    "        print(\"Skipping taxonomic group analysis due to missing class data\")\n",
    "        group_df = None\n",
    "\n",
    "    # Analyze confidence thresholds (with fewer thresholds)\n",
    "    print(\"\\nAnalyzing impact of confidence thresholds...\")\n",
    "    conf_df = analyze_confidence_thresholds(model, data_yaml, thresholds=[0.25, 0.5])\n",
    "\n",
    "    # Visualize sample predictions (reduced number)\n",
    "    print(\"\\nVisualizing sample predictions...\")\n",
    "    val_img_folder = os.path.join(os.path.dirname(data_yaml), 'images/val')\n",
    "    visualize_sample_predictions(model, val_img_folder, num_samples=2, conf=0.25)\n",
    "\n",
    "    # Generate simple report\n",
    "    print(\"\\nGenerating model analysis report...\")\n",
    "\n",
    "    # Safe string formatting for potentially numpy values\n",
    "    def safe_format(value):\n",
    "        try:\n",
    "            return f\"{float(value):.4f}\"\n",
    "        except (TypeError, ValueError):\n",
    "            if hasattr(value, 'mean'):\n",
    "                return f\"{float(value.mean()):.4f}\"\n",
    "            else:\n",
    "                return \"N/A\"\n",
    "\n",
    "    report = f\"\"\"# Wildlife Detection Model Analysis Report\n",
    "Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "\n",
    "## Model Information\n",
    "- Model: {model_path}\n",
    "- Data: {data_yaml}\n",
    "\n",
    "## Overall Performance Metrics\n",
    "- mAP50: {safe_format(map50)}\n",
    "- mAP50-95: {safe_format(map50_95)}\n",
    "- Precision: {safe_format(precision)}\n",
    "- Recall: {safe_format(recall)}\n",
    "\n",
    "## Confidence Threshold Analysis\n",
    "Threshold | mAP50 | Precision | Recall\n",
    "----------|-------|-----------|-------\n",
    "\"\"\"\n",
    "\n",
    "    # Add confidence threshold data if available\n",
    "    if conf_df is not None and len(conf_df) > 0:\n",
    "        for _, row in conf_df.iterrows():\n",
    "            report += f\"{row['Threshold']:.2f} | {row['mAP50']:.4f} | {row['Precision']:.4f} | {row['Recall']:.4f}\\n\"\n",
    "\n",
    "    # Add recommendations\n",
    "    report += \"\"\"\n",
    "## Key Observations\n",
    "\n",
    "1. The model performs best on Human and Male Roe Deer classes, likely due to their distinctive features and consistent appearance.\n",
    "\n",
    "2. Some classes with few samples (Weasel, Wildcat) show poor performance, indicating a need for more training data.\n",
    "\n",
    "3. Small animals like Rabbit show good detection rates, but their small size in images may lead to lower precision.\n",
    "\n",
    "## Recommendations for Model Improvement\n",
    "\n",
    "1. **Address Class Imbalance**: \n",
    "   - Collect more data for underrepresented classes\n",
    "   - Use data augmentation techniques for rare species\n",
    "   - Consider transfer learning from similar species\n",
    "\n",
    "2. **Environmental Factors**: \n",
    "   - Add separate analysis for day/night conditions\n",
    "   - Consider vegetation density in performance evaluation\n",
    "   - Analyze distance effects on detection accuracy\n",
    "\n",
    "3. **Model Architecture Adjustments**:\n",
    "   - Try different input resolutions for small animals\n",
    "   - Experiment with different backbone networks\n",
    "   - Consider specialized models for taxonomic groups\n",
    "\n",
    "4. **Data Quality Improvements**:\n",
    "   - Fix corrupt JPEG issues in training data\n",
    "   - Improve annotation consistency\n",
    "   - Add metadata about environmental conditions\n",
    "\"\"\"\n",
    "\n",
    "    # Save report\n",
    "    with open(ANALYSIS_DIR / 'model_analysis_report.md', 'w') as f:\n",
    "        f.write(report)\n",
    "\n",
    "    print(f\"Analysis complete! Report saved to: {ANALYSIS_DIR / 'model_analysis_report.md'}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error during analysis: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68bb43a3-4e32-4a46-9978-f45805443195",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
