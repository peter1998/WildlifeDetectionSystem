{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e25023ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Environment Setup and Dependencies\n",
    "import os\n",
    "import sys\n",
    "import platform\n",
    "import time\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import json\n",
    "import yaml\n",
    "import shutil\n",
    "import random\n",
    "\n",
    "# Deep learning\n",
    "import torch\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# Data analysis\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix, classification_report, precision_recall_curve\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import seaborn as sns\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import cv2\n",
    "\n",
    "# Progress tracking\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Print Python and environment information\n",
    "print(f\"Python version: {platform.python_version()}\")\n",
    "print(f\"Platform: {platform.platform()}\")\n",
    "\n",
    "# Check for CUDA\n",
    "try:\n",
    "    import torch\n",
    "    print(f\"PyTorch version: {torch.__version__}\")\n",
    "    print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"CUDA version: {torch.version.cuda}\")\n",
    "        print(f\"GPU device: {torch.cuda.get_device_name(0)}\")\n",
    "        print(f\"Number of GPUs: {torch.cuda.device_count()}\")\n",
    "        print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / (1024**3):.2f} GB\")\n",
    "    else:\n",
    "        print(\"CUDA is not available - evaluation will use CPU\")\n",
    "except ImportError:\n",
    "    print(\"PyTorch is not installed - you'll need to install it with pip install torch torchvision\")\n",
    "\n",
    "# Check for other required libraries\n",
    "required_packages = ['numpy', 'matplotlib', 'pandas', 'ultralytics', 'seaborn', 'scikit-learn']\n",
    "for package in required_packages:\n",
    "    try:\n",
    "        module = __import__(package.replace('-', '_'))\n",
    "        print(f\"✅ {package} is installed (version: {module.__version__})\")\n",
    "    except ImportError:\n",
    "        print(f\"❌ {package} is NOT installed - use pip install {package}\")\n",
    "    except AttributeError:\n",
    "        print(f\"✅ {package} is installed (version unknown)\")\n",
    "\n",
    "# Manually set the project root path to ensure accuracy\n",
    "project_root = \"/home/peter/Desktop/TU PHD/WildlifeDetectionSystem\"\n",
    "print(f\"\\nProject root path: {project_root}\")\n",
    "\n",
    "# Output the current working directory for reference\n",
    "print(f\"Current working directory: {os.getcwd()}\")\n",
    "\n",
    "print(\"\\nEnvironment setup check complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44369824-f8aa-46cc-bec9-5dbe2466ce27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Load Training Results Configuration\n",
    "# Find and load the configuration generated by the model training notebook\n",
    "import os\n",
    "import json\n",
    "import yaml\n",
    "from datetime import datetime\n",
    "\n",
    "# Manually set the project root path to ensure accuracy - this makes the cell self-contained\n",
    "project_root = \"/home/peter/Desktop/TU PHD/WildlifeDetectionSystem\"\n",
    "print(f\"Project root path: {project_root}\")\n",
    "\n",
    "def find_latest_config(config_dir, prefix=\"training_config_\"):\n",
    "    \"\"\"Find the latest configuration file based on timestamp in filename\"\"\"\n",
    "    config_files = [f for f in os.listdir(config_dir) if f.startswith(prefix) and f.endswith('.json')]\n",
    "    if not config_files:\n",
    "        return None\n",
    "    \n",
    "    # Sort by timestamp (assuming format training_config_YYYYMMDD_HHMM.json)\n",
    "    latest_config = sorted(config_files, reverse=True)[0]\n",
    "    return os.path.join(config_dir, latest_config)\n",
    "\n",
    "# Define paths\n",
    "config_dir = os.path.join(project_root, \"config\")\n",
    "if not os.path.exists(config_dir):\n",
    "    print(f\"❌ Config directory not found: {config_dir}\")\n",
    "    print(\"Please run notebook 2 (model training) first\")\n",
    "else:\n",
    "    # Try to find the latest config file\n",
    "    latest_config = find_latest_config(config_dir)\n",
    "    \n",
    "    if latest_config and os.path.exists(latest_config):\n",
    "        print(f\"Found configuration from notebook 2: {latest_config}\")\n",
    "        \n",
    "        # Load configuration\n",
    "        with open(latest_config, 'r') as f:\n",
    "            training_config = json.load(f)\n",
    "        \n",
    "        # Extract key paths and parameters\n",
    "        timestamp = training_config[\"timestamp\"]\n",
    "        input_config = training_config[\"input\"]\n",
    "        \n",
    "        # Get class names and taxonomic groups\n",
    "        class_names = input_config[\"class_names\"]\n",
    "        taxonomic_groups = input_config[\"taxonomic_groups\"]\n",
    "        \n",
    "        # Get dataset paths\n",
    "        standard_dataset_path = input_config[\"standard_dataset\"]\n",
    "        hierarchical_dataset_path = input_config[\"hierarchical_dataset\"]\n",
    "        \n",
    "        # Get model paths\n",
    "        standard_model_path = training_config.get(\"standard_best_model_path\")\n",
    "        hierarchical_model_path = training_config.get(\"hierarchical_best_model_path\")\n",
    "        \n",
    "        # Get actual model paths if present\n",
    "        standard_model_dir = training_config.get(\"standard_model\", {}).get(\"actual_model_path\")\n",
    "        hierarchical_model_dir = training_config.get(\"hierarchical_model\", {}).get(\"actual_model_path\")\n",
    "        \n",
    "        print(f\"\\nLoaded configuration with timestamp: {timestamp}\")\n",
    "        print(f\"Number of classes: {len(class_names)}\")\n",
    "        print(f\"Number of taxonomic groups: {len(taxonomic_groups)}\")\n",
    "        \n",
    "        # Check if the model files exist\n",
    "        if standard_model_path and os.path.exists(standard_model_path):\n",
    "            print(f\"✅ Standard model exists: {standard_model_path}\")\n",
    "        elif standard_model_path:\n",
    "            print(f\"❌ Standard model not found: {standard_model_path}\")\n",
    "        else:\n",
    "            print(\"❓ Standard model path not found in configuration\")\n",
    "            \n",
    "            # Try to find the model in the expected location\n",
    "            if standard_model_dir:\n",
    "                potential_model_path = os.path.join(standard_model_dir, \"weights\", \"best.pt\")\n",
    "                if os.path.exists(potential_model_path):\n",
    "                    standard_model_path = potential_model_path\n",
    "                    print(f\"✅ Found standard model at: {standard_model_path}\")\n",
    "                else:\n",
    "                    print(f\"❌ Could not find standard model at expected location: {potential_model_path}\")\n",
    "        \n",
    "        if hierarchical_model_path and os.path.exists(hierarchical_model_path):\n",
    "            print(f\"✅ Hierarchical model exists: {hierarchical_model_path}\")\n",
    "        elif hierarchical_model_path:\n",
    "            print(f\"❌ Hierarchical model not found: {hierarchical_model_path}\")\n",
    "        else:\n",
    "            print(\"❓ Hierarchical model path not found in configuration\")\n",
    "            \n",
    "            # Try to find the model in the expected location\n",
    "            if hierarchical_model_dir:\n",
    "                potential_model_path = os.path.join(hierarchical_model_dir, \"weights\", \"best.pt\")\n",
    "                if os.path.exists(potential_model_path):\n",
    "                    hierarchical_model_path = potential_model_path\n",
    "                    print(f\"✅ Found hierarchical model at: {potential_model_path}\")\n",
    "                else:\n",
    "                    print(f\"❌ Could not find hierarchical model at expected location: {potential_model_path}\")\n",
    "    else:\n",
    "        print(f\"❌ Configuration from notebook 2 not found in {config_dir}\")\n",
    "        print(\"Please run notebook 2 (model training) first\")\n",
    "\n",
    "# Define the output paths for this notebook\n",
    "reports_dir = os.path.join(project_root, \"reports\")\n",
    "evaluation_dir = os.path.join(reports_dir, \"evaluation\")\n",
    "timestamp_now = datetime.now().strftime(\"%Y%m%d_%H%M\")\n",
    "\n",
    "# Create evaluation directory with timestamp\n",
    "evaluation_timestamp_dir = os.path.join(evaluation_dir, f\"evaluation_{timestamp_now}\")\n",
    "os.makedirs(evaluation_timestamp_dir, exist_ok=True)\n",
    "\n",
    "print(f\"\\nEvaluation results will be saved to: {evaluation_timestamp_dir}\")\n",
    "\n",
    "# Save the evaluation configuration for reference and tracking\n",
    "evaluation_config = {\n",
    "    \"notebook\": \"03_model_evaluation\",\n",
    "    \"timestamp\": timestamp_now,\n",
    "    \"input\": {\n",
    "        \"training_config\": latest_config,\n",
    "        \"standard_model\": standard_model_path,\n",
    "        \"hierarchical_model\": hierarchical_model_path,\n",
    "        \"standard_dataset\": standard_dataset_path,\n",
    "        \"hierarchical_dataset\": hierarchical_dataset_path,\n",
    "        \"class_names\": class_names,\n",
    "        \"taxonomic_groups\": taxonomic_groups\n",
    "    },\n",
    "    \"output\": {\n",
    "        \"evaluation_dir\": evaluation_timestamp_dir,\n",
    "        \"reports_dir\": reports_dir\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save configuration\n",
    "evaluation_config_path = os.path.join(config_dir, f\"evaluation_config_{timestamp_now}.json\")\n",
    "with open(evaluation_config_path, 'w') as f:\n",
    "    json.dump(evaluation_config, f, indent=2)\n",
    "\n",
    "print(f\"\\nEvaluation configuration saved to: {evaluation_config_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12d94d58-c9a3-4d37-b2bc-a0743943e6ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Model Loading and Basic Testing\n",
    "# Load both standard and hierarchical models for evaluation\n",
    "\n",
    "def load_and_inspect_model(model_path, model_type=\"standard\"):\n",
    "    \"\"\"Load a YOLOv8 model and print basic information\"\"\"\n",
    "    if not model_path or not os.path.exists(model_path):\n",
    "        print(f\"❌ {model_type.capitalize()} model path not found or invalid: {model_path}\")\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        print(f\"Loading {model_type} model from: {model_path}\")\n",
    "        model = YOLO(model_path)\n",
    "        \n",
    "        # Print model information\n",
    "        print(f\"\\n{model_type.capitalize()} Model Information:\")\n",
    "        print(f\"- Architecture: {model.model.name if hasattr(model.model, 'name') else 'YOLOv8'}\")\n",
    "        print(f\"- Number of classes: {model.model.nc}\")\n",
    "        print(f\"- Input size: {model.model.args.get('imgsz', 640)}px\")\n",
    "        \n",
    "        # Print class names if available\n",
    "        if hasattr(model, \"names\") and model.names:\n",
    "            print(f\"- Classes: {', '.join(list(model.names.values())[:5])}{'...' if len(model.names) > 5 else ''}\")\n",
    "        \n",
    "        return model\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {model_type} model: {e}\")\n",
    "        return None\n",
    "\n",
    "def test_model_on_sample(model, image_path, conf_threshold=0.25, model_type=\"standard\"):\n",
    "    \"\"\"Run a quick test of the model on a sample image\"\"\"\n",
    "    if not model:\n",
    "        print(f\"❌ {model_type.capitalize()} model not loaded.\")\n",
    "        return None\n",
    "    \n",
    "    if not os.path.exists(image_path):\n",
    "        print(f\"❌ Sample image not found: {image_path}\")\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        print(f\"\\nTesting {model_type} model on sample image: {os.path.basename(image_path)}\")\n",
    "        results = model.predict(source=image_path, conf=conf_threshold, verbose=False)\n",
    "        \n",
    "        if results and len(results) > 0:\n",
    "            print(f\"Detections: {len(results[0].boxes)}\")\n",
    "            \n",
    "            # Print the top 3 detections\n",
    "            boxes = results[0].boxes\n",
    "            if len(boxes) > 0:\n",
    "                print(\"\\nTop detections:\")\n",
    "                \n",
    "                # Sort by confidence\n",
    "                confidences = boxes.conf.cpu().numpy()\n",
    "                sorted_indices = np.argsort(confidences)[::-1][:3]  # Top 3\n",
    "                \n",
    "                for i, idx in enumerate(sorted_indices):\n",
    "                    # Get class name\n",
    "                    cls_id = int(boxes.cls[idx].item())\n",
    "                    cls_name = model.names[cls_id]\n",
    "                    \n",
    "                    # Get confidence\n",
    "                    conf = confidences[idx]\n",
    "                    \n",
    "                    print(f\"  {i+1}. {cls_name}: {conf:.2f}\")\n",
    "            \n",
    "            # Create a visualization directory\n",
    "            vis_dir = os.path.join(evaluation_timestamp_dir, \"visualizations\")\n",
    "            os.makedirs(vis_dir, exist_ok=True)\n",
    "            \n",
    "            # Save the visualization\n",
    "            result_path = os.path.join(vis_dir, f\"{model_type}_sample_detection.jpg\")\n",
    "            results[0].save(filename=result_path)\n",
    "            print(f\"Visualization saved to: {result_path}\")\n",
    "            \n",
    "            return results[0]\n",
    "        else:\n",
    "            print(\"No detections found in the sample image.\")\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error testing {model_type} model: {e}\")\n",
    "        return None\n",
    "\n",
    "# Load the standard model\n",
    "standard_model = load_and_inspect_model(standard_model_path, \"standard\")\n",
    "\n",
    "# Load the hierarchical model\n",
    "hierarchical_model = load_and_inspect_model(hierarchical_model_path, \"hierarchical\")\n",
    "\n",
    "# Find a sample image for testing\n",
    "def find_sample_image(dataset_path):\n",
    "    \"\"\"Find a sample image from the dataset validation set\"\"\"\n",
    "    val_images_dir = os.path.join(dataset_path, \"images\", \"val\")\n",
    "    if os.path.exists(val_images_dir):\n",
    "        image_files = [f for f in os.listdir(val_images_dir) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
    "        if image_files:\n",
    "            return os.path.join(val_images_dir, image_files[0])\n",
    "    \n",
    "    # If validation set not found, try raw images\n",
    "    raw_images_dir = os.path.join(project_root, \"data\", \"raw_images\")\n",
    "    if os.path.exists(raw_images_dir):\n",
    "        for root, dirs, files in os.walk(raw_images_dir):\n",
    "            image_files = [f for f in files if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
    "            if image_files:\n",
    "                return os.path.join(root, image_files[0])\n",
    "    \n",
    "    return None\n",
    "\n",
    "# Find a sample image\n",
    "sample_image_path = find_sample_image(standard_dataset_path)\n",
    "if not sample_image_path:\n",
    "    print(\"❌ No sample images found for testing.\")\n",
    "else:\n",
    "    print(f\"Found sample image: {sample_image_path}\")\n",
    "    \n",
    "    # Test both models on the sample image\n",
    "    if standard_model:\n",
    "        standard_results = test_model_on_sample(standard_model, sample_image_path, conf_threshold=0.25, model_type=\"standard\")\n",
    "    \n",
    "    if hierarchical_model:\n",
    "        hierarchical_results = test_model_on_sample(hierarchical_model, sample_image_path, conf_threshold=0.25, model_type=\"hierarchical\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "970ded54-9773-4fbe-8ed8-5e3a80ca1d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Comprehensive Model Evaluation\n",
    "# Evaluate both models on the validation set with detailed metrics\n",
    "\n",
    "def evaluate_model(model, dataset_path, conf_thresholds=[0.25], iou_threshold=0.7, model_type=\"standard\"):\n",
    "    \"\"\"Evaluate a model on the validation set at different confidence thresholds\"\"\"\n",
    "    if not model:\n",
    "        print(f\"❌ {model_type.capitalize()} model not loaded.\")\n",
    "        return None\n",
    "    \n",
    "    # Find the YAML file\n",
    "    yaml_path = os.path.join(dataset_path, \"data.yaml\")\n",
    "    if not os.path.exists(yaml_path):\n",
    "        print(f\"❌ YAML file not found: {yaml_path}\")\n",
    "        return None\n",
    "    \n",
    "    # Create results dictionary\n",
    "    results = {\n",
    "        \"model_type\": model_type,\n",
    "        \"dataset\": dataset_path,\n",
    "        \"thresholds\": {}\n",
    "    }\n",
    "    \n",
    "    print(f\"\\nEvaluating {model_type} model on validation set...\")\n",
    "    \n",
    "    # Use CUDA if available\n",
    "    device = 0 if torch.cuda.is_available() else 'cpu'\n",
    "    \n",
    "    # Evaluate at each confidence threshold\n",
    "    for conf_threshold in conf_thresholds:\n",
    "        print(f\"\\nEvaluating at confidence threshold: {conf_threshold}\")\n",
    "        \n",
    "        try:\n",
    "            # Run validation\n",
    "            val_results = model.val(\n",
    "                data=yaml_path,\n",
    "                conf=conf_threshold,\n",
    "                iou=iou_threshold,\n",
    "                verbose=True,\n",
    "                device=device\n",
    "            )\n",
    "            \n",
    "            # Extract key metrics\n",
    "            metrics = {\n",
    "                \"precision\": float(val_results.box.mp),\n",
    "                \"recall\": float(val_results.box.mr),\n",
    "                \"mAP50\": float(val_results.box.map50),\n",
    "                \"mAP50-95\": float(val_results.box.map),\n",
    "                \"val_images\": int(val_results.nc),\n",
    "                \"speed_ms\": float(val_results.speed['inference']) + float(val_results.speed['preprocess']) + float(val_results.speed['postprocess'])\n",
    "            }\n",
    "            \n",
    "            # Get class-specific metrics if available\n",
    "            if hasattr(val_results, 'class_metrics'):\n",
    "                class_metrics = val_results.class_metrics\n",
    "                metrics[\"class_metrics\"] = class_metrics\n",
    "            \n",
    "            # Store results for this threshold\n",
    "            results[\"thresholds\"][str(conf_threshold)] = metrics\n",
    "            \n",
    "            # Print summary\n",
    "            print(f\"\\nResults at threshold {conf_threshold}:\")\n",
    "            print(f\"- Precision: {metrics['precision']:.4f}\")\n",
    "            print(f\"- Recall: {metrics['recall']:.4f}\")\n",
    "            print(f\"- mAP50: {metrics['mAP50']:.4f}\")\n",
    "            print(f\"- mAP50-95: {metrics['mAP50-95']:.4f}\")\n",
    "            print(f\"- Speed: {metrics['speed_ms']:.2f} ms per image\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error evaluating at threshold {conf_threshold}: {e}\")\n",
    "            results[\"thresholds\"][str(conf_threshold)] = {\"error\": str(e)}\n",
    "    \n",
    "    return results\n",
    "\n",
    "def perform_threshold_analysis(model, dataset_path, model_type=\"standard\"):\n",
    "    \"\"\"Analyze model performance across a range of confidence thresholds\"\"\"\n",
    "    if not model:\n",
    "        print(f\"❌ {model_type.capitalize()} model not loaded.\")\n",
    "        return None\n",
    "    \n",
    "    # Define a range of thresholds\n",
    "    thresholds = [0.05, 0.1, 0.25, 0.5, 0.75]\n",
    "    \n",
    "    # Evaluate model at each threshold\n",
    "    print(f\"\\nPerforming threshold analysis for {model_type} model...\")\n",
    "    results = evaluate_model(model, dataset_path, conf_thresholds=thresholds, model_type=model_type)\n",
    "    \n",
    "    if results:\n",
    "        # Create a dataframe for easier plotting\n",
    "        threshold_data = []\n",
    "        for threshold, metrics in results[\"thresholds\"].items():\n",
    "            if \"error\" not in metrics:\n",
    "                threshold_data.append({\n",
    "                    \"threshold\": float(threshold),\n",
    "                    \"precision\": metrics[\"precision\"],\n",
    "                    \"recall\": metrics[\"recall\"],\n",
    "                    \"mAP50\": metrics[\"mAP50\"],\n",
    "                    \"mAP50-95\": metrics[\"mAP50-95\"],\n",
    "                    \"speed_ms\": metrics[\"speed_ms\"]\n",
    "                })\n",
    "        \n",
    "        threshold_df = pd.DataFrame(threshold_data)\n",
    "        \n",
    "        # Plot precision-recall vs threshold\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(threshold_df[\"threshold\"], threshold_df[\"precision\"], \"b-\", label=\"Precision\")\n",
    "        plt.plot(threshold_df[\"threshold\"], threshold_df[\"recall\"], \"r-\", label=\"Recall\")\n",
    "        plt.plot(threshold_df[\"threshold\"], threshold_df[\"mAP50\"], \"g-\", label=\"mAP50\")\n",
    "        plt.xlabel(\"Confidence Threshold\")\n",
    "        plt.ylabel(\"Metric Value\")\n",
    "        plt.title(f\"{model_type.capitalize()} Model - Metrics vs Confidence Threshold\")\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.legend()\n",
    "        \n",
    "        # Save the plot\n",
    "        plot_path = os.path.join(evaluation_timestamp_dir, f\"{model_type}_threshold_analysis.png\")\n",
    "        plt.savefig(plot_path)\n",
    "        print(f\"Threshold analysis plot saved to: {plot_path}\")\n",
    "        \n",
    "        # Add results to the output\n",
    "        results[\"threshold_df\"] = threshold_df.to_dict()\n",
    "        results[\"threshold_plot\"] = plot_path\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    return None\n",
    "\n",
    "# Evaluate standard model\n",
    "if standard_model and standard_dataset_path:\n",
    "    standard_eval_results = perform_threshold_analysis(standard_model, standard_dataset_path, \"standard\")\n",
    "else:\n",
    "    standard_eval_results = None\n",
    "    print(\"Skipping standard model evaluation due to missing model or dataset.\")\n",
    "\n",
    "# Evaluate hierarchical model\n",
    "if hierarchical_model and hierarchical_dataset_path:\n",
    "    hierarchical_eval_results = perform_threshold_analysis(hierarchical_model, hierarchical_dataset_path, \"hierarchical\")\n",
    "else:\n",
    "    hierarchical_eval_results = None\n",
    "    print(\"Skipping hierarchical model evaluation due to missing model or dataset.\")\n",
    "\n",
    "# Save evaluation results to the configuration\n",
    "if standard_eval_results:\n",
    "    evaluation_config[\"standard_eval_results\"] = standard_eval_results\n",
    "if hierarchical_eval_results:\n",
    "    evaluation_config[\"hierarchical_eval_results\"] = hierarchical_eval_results\n",
    "\n",
    "# Update evaluation config\n",
    "with open(evaluation_config_path, 'w') as f:\n",
    "    json.dump(evaluation_config, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9e0633b-3c88-4dbc-8bb2-74d39a5ce863",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Per-Class Performance Analysis\n",
    "# Analyze performance metrics for each class and taxonomic group\n",
    "\n",
    "def analyze_per_class_performance(model, dataset_path, conf_threshold=0.25, model_type=\"standard\", class_names=None, taxonomic_groups=None):\n",
    "    \"\"\"Analyze and visualize per-class performance metrics\"\"\"\n",
    "    if not model:\n",
    "        print(f\"❌ {model_type.capitalize()} model not loaded.\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"\\nAnalyzing per-class performance for {model_type} model...\")\n",
    "    \n",
    "    # Get validation results with per-class metrics\n",
    "    yaml_path = os.path.join(dataset_path, \"data.yaml\")\n",
    "    if not os.path.exists(yaml_path):\n",
    "        print(f\"❌ YAML file not found: {yaml_path}\")\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        # Run validation with per-class metrics enabled\n",
    "        device = 0 if torch.cuda.is_available() else 'cpu'\n",
    "        val_results = model.val(\n",
    "            data=yaml_path,\n",
    "            conf=conf_threshold,\n",
    "            verbose=False,\n",
    "            device=device\n",
    "        )\n",
    "        \n",
    "        # Format is different between standard (class names) and hierarchical (group names)\n",
    "        if model_type == \"standard\" and class_names:\n",
    "            names = class_names\n",
    "        elif model_type == \"hierarchical\" and taxonomic_groups:\n",
    "            names = list(taxonomic_groups.keys())\n",
    "        else:\n",
    "            names = list(model.names.values())\n",
    "        \n",
    "        # Extract per-class metrics\n",
    "        class_metrics = {}\n",
    "        \n",
    "        # Different approaches depending on YOLOv8 version\n",
    "        if hasattr(val_results, 'names') and val_results.names:\n",
    "            model_names = val_results.names\n",
    "            for i, name in model_names.items():\n",
    "                if i < len(val_results.box.cls_dict):\n",
    "                    class_metrics[name] = {\n",
    "                        \"precision\": float(val_results.box.cls_dict[i].precision), \n",
    "                        \"recall\": float(val_results.box.cls_dict[i].recall),\n",
    "                        \"mAP50\": float(val_results.box.cls_dict[i].ap50),\n",
    "                        \"mAP50-95\": float(val_results.box.cls_dict[i].ap)\n",
    "                    }\n",
    "        elif hasattr(val_results, 'box') and hasattr(val_results.box, 'ap_class_index'):\n",
    "            for i, idx in enumerate(val_results.box.ap_class_index):\n",
    "                if idx < len(names):\n",
    "                    name = names[idx]\n",
    "                    class_metrics[name] = {\n",
    "                        \"precision\": float(val_results.box.cls[i]),\n",
    "                        \"recall\": float(val_results.box.cls[i]),\n",
    "                        \"mAP50\": float(val_results.box.ap50[i]),\n",
    "                        \"mAP50-95\": float(val_results.box.ap[i])\n",
    "                    }\n",
    "        else:\n",
    "            # Use a more cautious approach - try to find metrics in val_results\n",
    "            # This is a fallback in case the YOLOv8 API changes\n",
    "            print(\"Warning: Unable to extract per-class metrics directly. Using alternate approach.\")\n",
    "            \n",
    "            # Use the names from the model\n",
    "            for i, name in model.names.items():\n",
    "                class_metrics[name] = {\n",
    "                    \"precision\": 0.0,\n",
    "                    \"recall\": 0.0,\n",
    "                    \"mAP50\": 0.0,\n",
    "                    \"mAP50-95\": 0.0\n",
    "                }\n",
    "        \n",
    "        # Create a dataframe for easier visualization\n",
    "        class_data = []\n",
    "        for name, metrics in class_metrics.items():\n",
    "            class_data.append({\n",
    "                \"class\": name,\n",
    "                \"precision\": metrics[\"precision\"],\n",
    "                \"recall\": metrics[\"recall\"],\n",
    "                \"mAP50\": metrics[\"mAP50\"],\n",
    "                \"mAP50-95\": metrics[\"mAP50-95\"]\n",
    "            })\n",
    "        \n",
    "        class_df = pd.DataFrame(class_data)\n",
    "        \n",
    "        # Sort by mAP50 for better visualization\n",
    "        class_df = class_df.sort_values(by=\"mAP50\", ascending=False)\n",
    "        \n",
    "        # Plot per-class mAP50\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        ax = sns.barplot(x=\"class\", y=\"mAP50\", data=class_df)\n",
    "        plt.xticks(rotation=90)\n",
    "        plt.xlabel(\"Class\")\n",
    "        plt.ylabel(\"mAP50\")\n",
    "        plt.title(f\"{model_type.capitalize()} Model - Per-Class mAP50\")\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Save the plot\n",
    "        plot_path = os.path.join(evaluation_timestamp_dir, f\"{model_type}_per_class_map.png\")\n",
    "        plt.savefig(plot_path)\n",
    "        print(f\"Per-class mAP50 plot saved to: {plot_path}\")\n",
    "        \n",
    "        # Plot precision vs recall by class\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        plt.scatter(class_df[\"recall\"], class_df[\"precision\"], s=100, alpha=0.7)\n",
    "        \n",
    "        # Annotate points with class names\n",
    "        for i, row in class_df.iterrows():\n",
    "            plt.annotate(row[\"class\"], (row[\"recall\"], row[\"precision\"]), \n",
    "                         xytext=(5, 5), textcoords=\"offset points\", fontsize=8)\n",
    "        \n",
    "        plt.xlabel(\"Recall\")\n",
    "        plt.ylabel(\"Precision\")\n",
    "        plt.title(f\"{model_type.capitalize()} Model - Precision vs Recall by Class\")\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.xlim(0, 1)\n",
    "        plt.ylim(0, 1)\n",
    "        \n",
    "        # Save the plot\n",
    "        plot_path = os.path.join(evaluation_timestamp_dir, f\"{model_type}_precision_recall_by_class.png\")\n",
    "        plt.savefig(plot_path)\n",
    "        print(f\"Precision-recall by class plot saved to: {plot_path}\")\n",
    "        \n",
    "        # Return results\n",
    "        results = {\n",
    "            \"model_type\": model_type,\n",
    "            \"class_metrics\": class_metrics,\n",
    "            \"class_df\": class_df.to_dict(),\n",
    "            \"per_class_map_plot\": os.path.join(evaluation_timestamp_dir, f\"{model_type}_per_class_map.png\"),\n",
    "            \"precision_recall_plot\": os.path.join(evaluation_timestamp_dir, f\"{model_type}_precision_recall_by_class.png\")\n",
    "        }\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error analyzing per-class performance: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "# Analyze per-class performance for standard model\n",
    "if standard_model and standard_dataset_path:\n",
    "    standard_class_results = analyze_per_class_performance(\n",
    "        standard_model, \n",
    "        standard_dataset_path, \n",
    "        conf_threshold=0.25, \n",
    "        model_type=\"standard\", \n",
    "        class_names=class_names\n",
    "    )\n",
    "else:\n",
    "    standard_class_results = None\n",
    "    print(\"Skipping standard model per-class analysis due to missing model or dataset.\")\n",
    "\n",
    "# Analyze per-class performance for hierarchical model\n",
    "if hierarchical_model and hierarchical_dataset_path:\n",
    "    hierarchical_class_results = analyze_per_class_performance(\n",
    "        hierarchical_model, \n",
    "        hierarchical_dataset_path, \n",
    "        conf_threshold=0.25, \n",
    "        model_type=\"hierarchical\", \n",
    "        taxonomic_groups=taxonomic_groups\n",
    "    )\n",
    "else:\n",
    "    hierarchical_class_results = None\n",
    "    print(\"Skipping hierarchical model per-class analysis due to missing model or dataset.\")\n",
    "\n",
    "# Save class evaluation results to the configuration\n",
    "if standard_class_results:\n",
    "    evaluation_config[\"standard_class_results\"] = standard_class_results\n",
    "if hierarchical_class_results:\n",
    "    evaluation_config[\"hierarchical_class_results\"] = hierarchical_class_results\n",
    "\n",
    "# Update evaluation config\n",
    "with open(evaluation_config_path, 'w') as f:\n",
    "    json.dump(evaluation_config, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3056f183-74b2-4547-adf7-d4acf2d9a411",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Confusion Matrix Analysis\n",
    "# Generate and visualize confusion matrices for both models\n",
    "\n",
    "def generate_confusion_matrix(model, dataset_path, conf_threshold=0.25, model_type=\"standard\", class_names=None, taxonomic_groups=None):\n",
    "    \"\"\"Generate and visualize a confusion matrix for the model\"\"\"\n",
    "    if not model:\n",
    "        print(f\"❌ {model_type.capitalize()} model not loaded.\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"\\nGenerating confusion matrix for {model_type} model...\")\n",
    "    \n",
    "    # Get validation set path\n",
    "    yaml_path = os.path.join(dataset_path, \"data.yaml\")\n",
    "    if not os.path.exists(yaml_path):\n",
    "        print(f\"❌ YAML file not found: {yaml_path}\")\n",
    "        return None\n",
    "    \n",
    "    # Find validation images and labels\n",
    "    val_images_dir = os.path.join(dataset_path, \"images\", \"val\")\n",
    "    val_labels_dir = os.path.join(dataset_path, \"labels\", \"val\")\n",
    "    \n",
    "    if not os.path.exists(val_images_dir) or not os.path.exists(val_labels_dir):\n",
    "        print(f\"❌ Validation images or labels directory not found.\")\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        # Get list of validation images\n",
    "        val_images = [f for f in os.listdir(val_images_dir) if f.lower().endswith((\".jpg\", \".jpeg\", \".png\"))]\n",
    "        \n",
    "        if not val_images:\n",
    "            print(\"❌ No validation images found.\")\n",
    "            return None\n",
    "        \n",
    "        # Set up lists for ground truth and predictions\n",
    "        y_true = []\n",
    "        y_pred = []\n",
    "        \n",
    "        # Format is different between standard (class names) and hierarchical (group names)\n",
    "        if model_type == \"standard\" and class_names:\n",
    "            names = class_names\n",
    "        elif model_type == \"hierarchical\" and taxonomic_groups:\n",
    "            names = list(taxonomic_groups.keys())\n",
    "        else:\n",
    "            names = list(model.names.values())\n",
    "        \n",
    "        # Process each validation image\n",
    "        print(f\"Processing {len(val_images)} validation images...\")\n",
    "        \n",
    "        for image_file in tqdm(val_images):\n",
    "            image_path = os.path.join(val_images_dir, image_file)\n",
    "            \n",
    "            # Get ground truth labels\n",
    "            label_file = os.path.splitext(image_file)[0] + \".txt\"\n",
    "            label_path = os.path.join(val_labels_dir, label_file)\n",
    "            \n",
    "            # Skip images without labels\n",
    "            if not os.path.exists(label_path):\n",
    "                continue\n",
    "            \n",
    "            # Read ground truth labels\n",
    "            with open(label_path, \"r\") as f:\n",
    "                gt_lines = f.readlines()\n",
    "            \n",
    "            # Extract class IDs from ground truth\n",
    "            gt_classes = []\n",
    "            for line in gt_lines:\n",
    "                parts = line.strip().split()\n",
    "                if len(parts) >= 5:\n",
    "                    cls_id = int(parts[0])\n",
    "                    gt_classes.append(cls_id)\n",
    "            \n",
    "            # Skip if no ground truth classes\n",
    "            if not gt_classes:\n",
    "                continue\n",
    "            \n",
    "            # Run prediction\n",
    "            results = model.predict(source=image_path, conf=conf_threshold, verbose=False)\n",
    "            \n",
    "            # Extract prediction classes\n",
    "            pred_classes = []\n",
    "            if results and len(results) > 0 and len(results[0].boxes) > 0:\n",
    "                boxes = results[0].boxes\n",
    "                for i in range(len(boxes)):\n",
    "                    cls_id = int(boxes.cls[i].item())\n",
    "                    pred_classes.append(cls_id)\n",
    "            \n",
    "            # Add to y_true and y_pred\n",
    "            y_true.extend(gt_classes)\n",
    "            \n",
    "            # If no predictions, add background class (or -1 for no detection)\n",
    "            if not pred_classes:\n",
    "                # Match the length of ground truth with \"no detection\" classes\n",
    "                y_pred.extend([-1] * len(gt_classes))\n",
    "            else:\n",
    "                # Use the predicted classes\n",
    "                # Note: This simple approach may not handle multiple objects perfectly\n",
    "                # For a more accurate confusion matrix, we would need to match predictions to ground truth\n",
    "                # based on IoU, but this is a simplified approach\n",
    "                while len(pred_classes) < len(gt_classes):\n",
    "                    pred_classes.append(-1)  # Pad with \"no detection\"\n",
    "                \n",
    "                # If we have more predictions than ground truth, take the top confidence ones\n",
    "                if len(pred_classes) > len(gt_classes):\n",
    "                    pred_classes = pred_classes[:len(gt_classes)]\n",
    "                \n",
    "                y_pred.extend(pred_classes)\n",
    "        \n",
    "        # Create the confusion matrix\n",
    "        if not y_true or not y_pred or len(y_true) != len(y_pred):\n",
    "            print(\"❌ Not enough data to create confusion matrix.\")\n",
    "            return None\n",
    "        \n",
    "        # Get unique classes\n",
    "        unique_classes = sorted(set(y_true + y_pred))\n",
    "        if -1 in unique_classes:\n",
    "            unique_classes.remove(-1)\n",
    "        \n",
    "        # Create confusion matrix\n",
    "        cm = confusion_matrix(\n",
    "            y_true, \n",
    "            y_pred, \n",
    "            labels=unique_classes\n",
    "        )\n",
    "        \n",
    "        # Get class names for labeling\n",
    "        class_labels = []\n",
    "        for cls_id in unique_classes:\n",
    "            if cls_id < len(model.names):\n",
    "                class_labels.append(model.names[cls_id])\n",
    "            else:\n",
    "                class_labels.append(f\"Class {cls_id}\")\n",
    "        \n",
    "        # Plot confusion matrix\n",
    "        plt.figure(figsize=(12, 10))\n",
    "        sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=class_labels, yticklabels=class_labels)\n",
    "        plt.xlabel(\"Predicted\")\n",
    "        plt.ylabel(\"Ground Truth\")\n",
    "        plt.title(f\"{model_type.capitalize()} Model - Confusion Matrix\")\n",
    "        plt.xticks(rotation=90)\n",
    "        plt.yticks(rotation=0)\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Save the plot\n",
    "        plot_path = os.path.join(evaluation_timestamp_dir, f\"{model_type}_confusion_matrix.png\")\n",
    "        plt.savefig(plot_path)\n",
    "        print(f\"Confusion matrix saved to: {plot_path}\")\n",
    "        \n",
    "        # Create a normalized confusion matrix\n",
    "        if np.sum(cm) > 0:\n",
    "            cm_norm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "            \n",
    "            # Handle division by zero\n",
    "            cm_norm = np.nan_to_num(cm_norm)\n",
    "            \n",
    "            # Plot normalized confusion matrix\n",
    "            plt.figure(figsize=(12, 10))\n",
    "            sns.heatmap(cm_norm, annot=True, fmt=\".2f\", cmap=\"Blues\", xticklabels=class_labels, yticklabels=class_labels)\n",
    "            plt.xlabel(\"Predicted\")\n",
    "            plt.ylabel(\"Ground Truth\")\n",
    "            plt.title(f\"{model_type.capitalize()} Model - Normalized Confusion Matrix\")\n",
    "            plt.xticks(rotation=90)\n",
    "            plt.yticks(rotation=0)\n",
    "            plt.tight_layout()\n",
    "            \n",
    "            # Save the plot\n",
    "            norm_plot_path = os.path.join(evaluation_timestamp_dir, f\"{model_type}_normalized_confusion_matrix.png\")\n",
    "            plt.savefig(norm_plot_path)\n",
    "            print(f\"Normalized confusion matrix saved to: {norm_plot_path}\")\n",
    "        \n",
    "        # Save confusion matrix data\n",
    "        confusion_data = {\n",
    "            \"confusion_matrix\": cm.tolist(),\n",
    "            \"normalized_confusion_matrix\": cm_norm.tolist() if np.sum(cm) > 0 else None,\n",
    "            \"class_labels\": class_labels,\n",
    "            \"confusion_matrix_plot\": plot_path,\n",
    "            \"normalized_confusion_matrix_plot\": norm_plot_path if np.sum(cm) > 0 else None\n",
    "        }\n",
    "        \n",
    "        # Save confusion matrix as a JSON file\n",
    "        json_path = os.path.join(evaluation_timestamp_dir, f\"{model_type}_confusion_matrix.json\")\n",
    "        with open(json_path, 'w') as f:\n",
    "            json.dump({\n",
    "                \"matrix\": cm.tolist(),\n",
    "                \"normalized_matrix\": cm_norm.tolist() if np.sum(cm) > 0 else None,\n",
    "                \"class_names\": class_labels\n",
    "            }, f, indent=2)\n",
    "        \n",
    "        print(f\"Confusion matrix data saved to: {json_path}\")\n",
    "        \n",
    "        return confusion_data\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error generating confusion matrix: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "# Generate confusion matrix for standard model\n",
    "if standard_model and standard_dataset_path:\n",
    "    standard_confusion = generate_confusion_matrix(\n",
    "        standard_model, \n",
    "        standard_dataset_path, \n",
    "        conf_threshold=0.25, \n",
    "        model_type=\"standard\", \n",
    "        class_names=class_names\n",
    "    )\n",
    "else:\n",
    "    standard_confusion = None\n",
    "    print(\"Skipping standard model confusion matrix due to missing model or dataset.\")\n",
    "\n",
    "# Generate confusion matrix for hierarchical model\n",
    "if hierarchical_model and hierarchical_dataset_path:\n",
    "    hierarchical_confusion = generate_confusion_matrix(\n",
    "        hierarchical_model, \n",
    "        hierarchical_dataset_path, \n",
    "        conf_threshold=0.25, \n",
    "        model_type=\"hierarchical\", \n",
    "        taxonomic_groups=taxonomic_groups\n",
    "    )\n",
    "else:\n",
    "    hierarchical_confusion = None\n",
    "    print(\"Skipping hierarchical model confusion matrix due to missing model or dataset.\")\n",
    "\n",
    "# Save confusion matrices to the configuration\n",
    "if standard_confusion:\n",
    "    evaluation_config[\"standard_confusion\"] = standard_confusion\n",
    "if hierarchical_confusion:\n",
    "    evaluation_config[\"hierarchical_confusion\"] = hierarchical_confusion\n",
    "\n",
    "# Update evaluation config\n",
    "with open(evaluation_config_path, 'w') as f:\n",
    "    json.dump(evaluation_config, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91240626-ffb3-4578-b0da-4380f0f0c4dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Error Analysis and Visualization\n",
    "# Analyze and visualize detection errors for both models\n",
    "\n",
    "def analyze_detection_errors(model, dataset_path, conf_threshold=0.25, model_type=\"standard\", max_examples=5):\n",
    "    \"\"\"Find and visualize detection errors (false positives and false negatives)\"\"\"\n",
    "    if not model:\n",
    "        print(f\"❌ {model_type.capitalize()} model not loaded.\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"\\nAnalyzing detection errors for {model_type} model...\")\n",
    "    \n",
    "    # Get validation set path\n",
    "    yaml_path = os.path.join(dataset_path, \"data.yaml\")\n",
    "    if not os.path.exists(yaml_path):\n",
    "        print(f\"❌ YAML file not found: {yaml_path}\")\n",
    "        return None\n",
    "    \n",
    "    # Find validation images and labels\n",
    "    val_images_dir = os.path.join(dataset_path, \"images\", \"val\")\n",
    "    val_labels_dir = os.path.join(dataset_path, \"labels\", \"val\")\n",
    "    \n",
    "    if not os.path.exists(val_images_dir) or not os.path.exists(val_labels_dir):\n",
    "        print(f\"❌ Validation images or labels directory not found.\")\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        # Get list of validation images\n",
    "        val_images = [f for f in os.listdir(val_images_dir) if f.lower().endswith((\".jpg\", \".jpeg\", \".png\"))]\n",
    "        \n",
    "        if not val_images:\n",
    "            print(\"❌ No validation images found.\")\n",
    "            return None\n",
    "        \n",
    "        # Set up error tracking\n",
    "        false_positives = []\n",
    "        false_negatives = []\n",
    "        \n",
    "        # Process each validation image\n",
    "        print(f\"Processing {len(val_images)} validation images for errors...\")\n",
    "        \n",
    "        for image_file in tqdm(val_images):\n",
    "            image_path = os.path.join(val_images_dir, image_file)\n",
    "            \n",
    "            # Get ground truth labels\n",
    "            label_file = os.path.splitext(image_file)[0] + \".txt\"\n",
    "            label_path = os.path.join(val_labels_dir, label_file)\n",
    "            \n",
    "            # If no label file, all predictions are false positives\n",
    "            if not os.path.exists(label_path):\n",
    "                # Run prediction\n",
    "                results = model.predict(source=image_path, conf=conf_threshold, verbose=False)\n",
    "                \n",
    "                # Check if there are any predictions\n",
    "                if results and len(results) > 0 and len(results[0].boxes) > 0:\n",
    "                    false_positives.append((image_path, results[0]))\n",
    "                \n",
    "                continue\n",
    "            \n",
    "            # Read ground truth labels\n",
    "            with open(label_path, \"r\") as f:\n",
    "                gt_lines = f.readlines()\n",
    "            \n",
    "            # Parse ground truth\n",
    "            gt_boxes = []\n",
    "            for line in gt_lines:\n",
    "                parts = line.strip().split()\n",
    "                if len(parts) >= 5:\n",
    "                    class_id = int(parts[0])\n",
    "                    x_center = float(parts[1])\n",
    "                    y_center = float(parts[2])\n",
    "                    width = float(parts[3])\n",
    "                    height = float(parts[4])\n",
    "                    \n",
    "                    # Convert to absolute pixel coordinates later when we know the image size\n",
    "                    gt_boxes.append((class_id, x_center, y_center, width, height))\n",
    "            \n",
    "            # Run prediction\n",
    "            results = model.predict(source=image_path, conf=conf_threshold, verbose=False)\n",
    "            \n",
    "            # Check for errors\n",
    "            if not results or len(results) == 0 or len(results[0].boxes) == 0:\n",
    "                # No detections, but ground truth exists - false negative\n",
    "                if gt_boxes:\n",
    "                    false_negatives.append((image_path, None, gt_boxes))\n",
    "            else:\n",
    "                # Get predicted boxes\n",
    "                pred_boxes = results[0].boxes\n",
    "                \n",
    "                # Check for false positives and false negatives\n",
    "                # This is a simplified approach - a more accurate approach would use IoU matching\n",
    "                # For simplicity, we'll just compare the number of predictions to ground truth\n",
    "                if len(pred_boxes) > len(gt_boxes):\n",
    "                    # More predictions than ground truth - potential false positives\n",
    "                    false_positives.append((image_path, results[0]))\n",
    "                elif len(pred_boxes) < len(gt_boxes):\n",
    "                    # Fewer predictions than ground truth - potential false negatives\n",
    "                    false_negatives.append((image_path, results[0], gt_boxes))\n",
    "        \n",
    "        # Create an error analysis directory\n",
    "        error_dir = os.path.join(evaluation_timestamp_dir, f\"{model_type}_error_analysis\")\n",
    "        os.makedirs(error_dir, exist_ok=True)\n",
    "        \n",
    "        # Visualize false positives\n",
    "        fp_examples = []\n",
    "        print(f\"\\nFound {len(false_positives)} images with potential false positives.\")\n",
    "        \n",
    "        for i, (image_path, result) in enumerate(false_positives[:max_examples]):\n",
    "            print(f\"Visualizing false positive example {i+1}/{min(max_examples, len(false_positives))}\")\n",
    "            \n",
    "            # Save the visualization\n",
    "            vis_path = os.path.join(error_dir, f\"false_positive_{i+1}.jpg\")\n",
    "            \n",
    "            # Use YOLO's built-in visualization\n",
    "            result.save(filename=vis_path)\n",
    "            \n",
    "            # Add to examples\n",
    "            fp_examples.append({\n",
    "                \"image_path\": image_path,\n",
    "                \"visualization\": vis_path\n",
    "            })\n",
    "        \n",
    "        # Visualize false negatives\n",
    "        fn_examples = []\n",
    "        print(f\"\\nFound {len(false_negatives)} images with potential false negatives.\")\n",
    "        \n",
    "        for i, (image_path, result, gt_boxes) in enumerate(false_negatives[:max_examples]):\n",
    "            print(f\"Visualizing false negative example {i+1}/{min(max_examples, len(false_negatives))}\")\n",
    "            \n",
    "            # Save the visualization\n",
    "            vis_path = os.path.join(error_dir, f\"false_negative_{i+1}.jpg\")\n",
    "            \n",
    "            # Load the image\n",
    "            image = cv2.imread(image_path)\n",
    "            if image is None:\n",
    "                print(f\"Error loading image: {image_path}\")\n",
    "                continue\n",
    "            \n",
    "            # Convert BGR to RGB\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "            \n",
    "            # Get image dimensions\n",
    "            img_height, img_width = image.shape[:2]\n",
    "            \n",
    "            # Draw ground truth boxes in green\n",
    "            for cls_id, x_center, y_center, width, height in gt_boxes:\n",
    "                # Convert normalized coordinates to pixels\n",
    "                x1 = int((x_center - width/2) * img_width)\n",
    "                y1 = int((y_center - height/2) * img_height)\n",
    "                x2 = int((x_center + width/2) * img_width)\n",
    "                y2 = int((y_center + height/2) * img_height)\n",
    "                \n",
    "                # Draw the box\n",
    "                cv2.rectangle(image, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "                \n",
    "                # Draw the label\n",
    "                if cls_id in model.names:\n",
    "                    cls_name = model.names[cls_id]\n",
    "                else:\n",
    "                    cls_name = f\"Class {cls_id}\"\n",
    "                \n",
    "                cv2.putText(image, cls_name, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "            \n",
    "            # If there are predictions, draw them in red\n",
    "            if result is not None and len(result.boxes) > 0:\n",
    "                boxes = result.boxes\n",
    "                for i in range(len(boxes)):\n",
    "                    # Get coordinates\n",
    "                    x1, y1, x2, y2 = boxes.xyxy[i].cpu().numpy().astype(int)\n",
    "                    \n",
    "                    # Draw the box\n",
    "                    cv2.rectangle(image, (x1, y1), (x2, y2), (255, 0, 0), 2)\n",
    "                    \n",
    "                    # Get class name\n",
    "                    cls_id = int(boxes.cls[i].item())\n",
    "                    if cls_id in model.names:\n",
    "                        cls_name = model.names[cls_id]\n",
    "                    else:\n",
    "                        cls_name = f\"Class {cls_id}\"\n",
    "                    \n",
    "                    # Get confidence\n",
    "                    conf = boxes.conf[i].item()\n",
    "                    \n",
    "                    # Draw the label\n",
    "                    cv2.putText(image, f\"{cls_name} {conf:.2f}\", (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0), 2)\n",
    "            \n",
    "            # Save the image\n",
    "            plt.figure(figsize=(10, 10))\n",
    "            plt.imshow(image)\n",
    "            plt.axis(\"off\")\n",
    "            plt.title(f\"False Negative Example - Red: Predictions, Green: Ground Truth\")\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(vis_path)\n",
    "            plt.close()\n",
    "            \n",
    "            # Add to examples\n",
    "            fn_examples.append({\n",
    "                \"image_path\": image_path,\n",
    "                \"visualization\": vis_path\n",
    "            })\n",
    "        \n",
    "        # Save error analysis report\n",
    "        report = {\n",
    "            \"model_type\": model_type,\n",
    "            \"false_positives\": {\n",
    "                \"count\": len(false_positives),\n",
    "                \"examples\": fp_examples\n",
    "            },\n",
    "            \"false_negatives\": {\n",
    "                \"count\": len(false_negatives),\n",
    "                \"examples\": fn_examples\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Create a markdown report\n",
    "        report_path = os.path.join(evaluation_timestamp_dir, f\"{model_type}_error_analysis.md\")\n",
    "        with open(report_path, \"w\") as f:\n",
    "            f.write(f\"# {model_type.capitalize()} Model Error Analysis\\n\\n\")\n",
    "            f.write(f\"Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\\n\")\n",
    "            \n",
    "            f.write(f\"## False Positives\\n\\n\")\n",
    "            f.write(f\"Total images with potential false positives: {len(false_positives)}\\n\\n\")\n",
    "            \n",
    "            for i, example in enumerate(fp_examples):\n",
    "                f.write(f\"### Example {i+1}\\n\\n\")\n",
    "                f.write(f\"Image: {os.path.basename(example['image_path'])}\\n\\n\")\n",
    "                f.write(f\"![False Positive]({os.path.relpath(example['visualization'], evaluation_timestamp_dir)})\\n\\n\")\n",
    "            \n",
    "            f.write(f\"## False Negatives\\n\\n\")\n",
    "            f.write(f\"Total images with potential false negatives: {len(false_negatives)}\\n\\n\")\n",
    "            \n",
    "            for i, example in enumerate(fn_examples):\n",
    "                f.write(f\"### Example {i+1}\\n\\n\")\n",
    "                f.write(f\"Image: {os.path.basename(example['image_path'])}\\n\\n\")\n",
    "                f.write(f\"![False Negative]({os.path.relpath(example['visualization'], evaluation_timestamp_dir)})\\n\\n\")\n",
    "        \n",
    "        print(f\"Error analysis report saved to: {report_path}\")\n",
    "        \n",
    "        # Return the report\n",
    "        report[\"report_path\"] = report_path\n",
    "        return report\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error analyzing detection errors: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "# Analyze detection errors for standard model\n",
    "if standard_model and standard_dataset_path:\n",
    "    standard_error_analysis = analyze_detection_errors(\n",
    "        standard_model, \n",
    "        standard_dataset_path, \n",
    "        conf_threshold=0.25, \n",
    "        model_type=\"standard\", \n",
    "        max_examples=5\n",
    "    )\n",
    "else:\n",
    "    standard_error_analysis = None\n",
    "    print(\"Skipping standard model error analysis due to missing model or dataset.\")\n",
    "\n",
    "# Analyze detection errors for hierarchical model\n",
    "if hierarchical_model and hierarchical_dataset_path:\n",
    "    hierarchical_error_analysis = analyze_detection_errors(\n",
    "        hierarchical_model, \n",
    "        hierarchical_dataset_path, \n",
    "        conf_threshold=0.25, \n",
    "        model_type=\"hierarchical\", \n",
    "        max_examples=5\n",
    "    )\n",
    "else:\n",
    "    hierarchical_error_analysis = None\n",
    "    print(\"Skipping hierarchical model error analysis due to missing model or dataset.\")\n",
    "\n",
    "# Save error analysis to the configuration\n",
    "if standard_error_analysis:\n",
    "    evaluation_config[\"standard_error_analysis\"] = standard_error_analysis\n",
    "if hierarchical_error_analysis:\n",
    "    evaluation_config[\"hierarchical_error_analysis\"] = hierarchical_error_analysis\n",
    "\n",
    "# Update evaluation config\n",
    "with open(evaluation_config_path, 'w') as f:\n",
    "    json.dump(evaluation_config, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dde5e152-3862-49d2-9da0-b721aa5dd1dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Model Comparison and Summary\n",
    "# Compare standard and hierarchical models and generate a comprehensive report\n",
    "\n",
    "def compare_models(standard_results, hierarchical_results):\n",
    "    \"\"\"Compare the performance of standard and hierarchical models\"\"\"\n",
    "    if not standard_results or not hierarchical_results:\n",
    "        print(\"❌ Insufficient data for model comparison.\")\n",
    "        return None\n",
    "    \n",
    "    print(\"Comparing standard and hierarchical models...\")\n",
    "    \n",
    "    # Extract key metrics\n",
    "    metrics = [\"precision\", \"recall\", \"mAP50\", \"mAP50-95\"]\n",
    "    thresholds = [\"0.25\", \"0.5\"]  # Common thresholds to compare\n",
    "    \n",
    "    comparison_data = {}\n",
    "    for threshold in thresholds:\n",
    "        if threshold in standard_results[\"thresholds\"] and threshold in hierarchical_results[\"thresholds\"]:\n",
    "            std_metrics = standard_results[\"thresholds\"][threshold]\n",
    "            hier_metrics = hierarchical_results[\"thresholds\"][threshold]\n",
    "            \n",
    "            threshold_data = {}\n",
    "            for metric in metrics:\n",
    "                if metric in std_metrics and metric in hier_metrics:\n",
    "                    std_value = std_metrics[metric]\n",
    "                    hier_value = hier_metrics[metric]\n",
    "                    \n",
    "                    # Calculate improvement\n",
    "                    if std_value > 0:\n",
    "                        improvement = (hier_value - std_value) / std_value * 100\n",
    "                    else:\n",
    "                        improvement = float('inf') if hier_value > 0 else 0\n",
    "                    \n",
    "                    threshold_data[metric] = {\n",
    "                        \"standard\": std_value,\n",
    "                        \"hierarchical\": hier_value,\n",
    "                        \"improvement\": improvement,\n",
    "                        \"improvement_percent\": f\"{improvement:.1f}%\"\n",
    "                    }\n",
    "            \n",
    "            comparison_data[threshold] = threshold_data\n",
    "    \n",
    "    # Create bar chart to visualize improvement\n",
    "    if comparison_data:\n",
    "        # Use the first threshold for visualization\n",
    "        threshold = list(comparison_data.keys())[0]\n",
    "        \n",
    "        # Prepare data for plotting\n",
    "        plot_data = []\n",
    "        for metric in metrics:\n",
    "            if metric in comparison_data[threshold]:\n",
    "                std_value = comparison_data[threshold][metric][\"standard\"]\n",
    "                hier_value = comparison_data[threshold][metric][\"hierarchical\"]\n",
    "                improvement = comparison_data[threshold][metric][\"improvement\"]\n",
    "                \n",
    "                plot_data.append({\n",
    "                    \"metric\": metric,\n",
    "                    \"standard\": std_value,\n",
    "                    \"hierarchical\": hier_value,\n",
    "                    \"improvement\": improvement\n",
    "                })\n",
    "        \n",
    "        # Convert to dataframe\n",
    "        df = pd.DataFrame(plot_data)\n",
    "        \n",
    "        # Create plot\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        \n",
    "        # Bar width\n",
    "        width = 0.35\n",
    "        \n",
    "        # Set up bars\n",
    "        x = np.arange(len(metrics))\n",
    "        \n",
    "        # Create bars\n",
    "        plt.bar(x - width/2, df[\"standard\"], width, label=\"Standard\")\n",
    "        plt.bar(x + width/2, df[\"hierarchical\"], width, label=\"Hierarchical\")\n",
    "        \n",
    "        # Add labels and title\n",
    "        plt.xlabel(\"Metric\")\n",
    "        plt.ylabel(\"Value\")\n",
    "        plt.title(f\"Model Comparison (Confidence Threshold = {threshold})\")\n",
    "        plt.xticks(x, df[\"metric\"])\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Add improvement percentage as text\n",
    "        for i, row in df.iterrows():\n",
    "            plt.text(i, max(row[\"standard\"], row[\"hierarchical\"]) + 0.02, \n",
    "                     f\"{row['improvement']:.1f}%\", \n",
    "                     ha=\"center\", va=\"bottom\", fontweight=\"bold\")\n",
    "        \n",
    "        # Save the plot\n",
    "        plot_path = os.path.join(evaluation_timestamp_dir, \"model_comparison.png\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(plot_path)\n",
    "        print(f\"Model comparison plot saved to: {plot_path}\")\n",
    "        \n",
    "        # Create a comparison report\n",
    "        report_path = os.path.join(evaluation_timestamp_dir, \"model_comparison.md\")\n",
    "        with open(report_path, \"w\") as f:\n",
    "            f.write(\"# Wildlife Detection Model Comparison\\n\\n\")\n",
    "            f.write(f\"Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\\n\")\n",
    "            \n",
    "            f.write(\"## Performance Comparison\\n\\n\")\n",
    "            f.write(\"### Standard vs. Hierarchical Model\\n\\n\")\n",
    "            \n",
    "            # Add image\n",
    "            f.write(f\"![Model Comparison](model_comparison.png)\\n\\n\")\n",
    "            \n",
    "            # Detailed metrics table\n",
    "            f.write(\"### Detailed Metrics\\n\\n\")\n",
    "            \n",
    "            for threshold in comparison_data:\n",
    "                f.write(f\"#### Confidence Threshold = {threshold}\\n\\n\")\n",
    "                f.write(\"| Metric | Standard | Hierarchical | Improvement |\\n\")\n",
    "                f.write(\"|--------|----------|--------------|-------------|\\n\")\n",
    "                \n",
    "                for metric in metrics:\n",
    "                    if metric in comparison_data[threshold]:\n",
    "                        data = comparison_data[threshold][metric]\n",
    "                        f.write(f\"| {metric} | {data['standard']:.4f} | {data['hierarchical']:.4f} | {data['improvement_percent']} |\\n\")\n",
    "                \n",
    "                f.write(\"\\n\")\n",
    "            \n",
    "            # Add a key findings section\n",
    "            f.write(\"## Key Findings\\n\\n\")\n",
    "            \n",
    "            # Determine if hierarchical model is better overall\n",
    "            better_metrics = sum(1 for threshold in comparison_data for metric in metrics \n",
    "                               if metric in comparison_data[threshold] and \n",
    "                               comparison_data[threshold][metric][\"improvement\"] > 0)\n",
    "            total_metrics = sum(1 for threshold in comparison_data for metric in metrics \n",
    "                             if metric in comparison_data[threshold])\n",
    "            \n",
    "            if better_metrics / total_metrics > 0.5:\n",
    "                f.write(\"The **hierarchical model outperforms the standard model** in the majority of metrics. \")\n",
    "                \n",
    "                # Find the most improved metric\n",
    "                best_improvement = 0\n",
    "                best_metric = \"\"\n",
    "                best_threshold = \"\"\n",
    "                \n",
    "                for threshold in comparison_data:\n",
    "                    for metric in metrics:\n",
    "                        if metric in comparison_data[threshold]:\n",
    "                            improvement = comparison_data[threshold][metric][\"improvement\"]\n",
    "                            if improvement > best_improvement:\n",
    "                                best_improvement = improvement\n",
    "                                best_metric = metric\n",
    "                                best_threshold = threshold\n",
    "                \n",
    "                if best_metric:\n",
    "                    f.write(f\"The most significant improvement is in **{best_metric}** at threshold {best_threshold}, \")\n",
    "                    f.write(f\"with an increase of **{best_improvement:.1f}%**.\\n\\n\")\n",
    "                \n",
    "                f.write(\"This supports the hypothesis that grouping species into taxonomic categories \")\n",
    "                f.write(\"improves detection performance, especially for wildlife with limited training examples.\\n\\n\")\n",
    "            else:\n",
    "                f.write(\"The results are mixed, with the hierarchical model performing better in some metrics \")\n",
    "                f.write(\"but the standard model performing better in others.\\n\\n\")\n",
    "            \n",
    "            # Add recommendations\n",
    "            f.write(\"## Recommendations\\n\\n\")\n",
    "            \n",
    "            if better_metrics / total_metrics > 0.7:\n",
    "                f.write(\"1. **Adopt the hierarchical approach** for wildlife detection as the primary method.\\n\")\n",
    "                f.write(\"2. Consider a two-stage detection pipeline, where the hierarchical model identifies the taxonomic group, \")\n",
    "                f.write(\"followed by a specialized model to identify the specific species within that group.\\n\")\n",
    "            else:\n",
    "                f.write(\"1. Use the **hierarchical model** for general wildlife detection and classification into taxonomic groups.\\n\")\n",
    "                f.write(\"2. Use the **standard model** for specific species identification when high confidence is required.\\n\")\n",
    "            \n",
    "            f.write(\"3. Continue collecting additional training data, particularly for rare species with few examples.\\n\")\n",
    "            f.write(\"4. Explore model ensembling techniques to combine the strengths of both approaches.\\n\")\n",
    "        \n",
    "        print(f\"Model comparison report saved to: {report_path}\")\n",
    "        \n",
    "        # Return the comparison data\n",
    "        return {\n",
    "            \"comparison_data\": comparison_data,\n",
    "            \"plot_path\": plot_path,\n",
    "            \"report_path\": report_path\n",
    "        }\n",
    "    else:\n",
    "        print(\"❌ No common thresholds found for comparison.\")\n",
    "        return None\n",
    "\n",
    "def generate_comprehensive_report():\n",
    "    \"\"\"Generate a comprehensive evaluation report combining all analyses\"\"\"\n",
    "    print(\"\\nGenerating comprehensive evaluation report...\")\n",
    "    \n",
    "    # Create the report path\n",
    "    report_path = os.path.join(evaluation_timestamp_dir, \"comprehensive_evaluation_report.md\")\n",
    "    \n",
    "    # Extract key information\n",
    "    standard_model_info = None\n",
    "    hierarchical_model_info = None\n",
    "    \n",
    "    if standard_model:\n",
    "        standard_model_info = {\n",
    "            \"path\": standard_model_path,\n",
    "            \"classes\": len(class_names) if class_names else 0\n",
    "        }\n",
    "    \n",
    "    if hierarchical_model:\n",
    "        hierarchical_model_info = {\n",
    "            \"path\": hierarchical_model_path,\n",
    "            \"groups\": len(taxonomic_groups) if taxonomic_groups else 0\n",
    "        }\n",
    "    \n",
    "    # Write the report\n",
    "    with open(report_path, \"w\") as f:\n",
    "        f.write(\"# Wildlife Detection System - Comprehensive Evaluation Report\\n\\n\")\n",
    "        f.write(f\"Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\\n\")\n",
    "        \n",
    "        # Models section\n",
    "        f.write(\"## Models Evaluated\\n\\n\")\n",
    "        \n",
    "        if standard_model_info:\n",
    "            f.write(\"### Standard Model\\n\\n\")\n",
    "            f.write(f\"- Model Path: `{standard_model_info['path']}`\\n\")\n",
    "            f.write(f\"- Number of Classes: {standard_model_info['classes']}\\n\")\n",
    "            if 'standard_eval_results' in evaluation_config and evaluation_config['standard_eval_results']:\n",
    "                threshold_key = \"0.25\"  # Use a common threshold\n",
    "                if threshold_key in evaluation_config['standard_eval_results'][\"thresholds\"]:\n",
    "                    metrics = evaluation_config['standard_eval_results'][\"thresholds\"][threshold_key]\n",
    "                    f.write(f\"- Precision: {metrics['precision']:.4f}\\n\")\n",
    "                    f.write(f\"- Recall: {metrics['recall']:.4f}\\n\")\n",
    "                    f.write(f\"- mAP50: {metrics['mAP50']:.4f}\\n\")\n",
    "                    f.write(f\"- mAP50-95: {metrics['mAP50-95']:.4f}\\n\")\n",
    "            f.write(\"\\n\")\n",
    "        \n",
    "        if hierarchical_model_info:\n",
    "            f.write(\"### Hierarchical Model\\n\\n\")\n",
    "            f.write(f\"- Model Path: `{hierarchical_model_info['path']}`\\n\")\n",
    "            f.write(f\"- Number of Taxonomic Groups: {hierarchical_model_info['groups']}\\n\")\n",
    "            if 'hierarchical_eval_results' in evaluation_config and evaluation_config['hierarchical_eval_results']:\n",
    "                threshold_key = \"0.25\"  # Use a common threshold\n",
    "                if threshold_key in evaluation_config['hierarchical_eval_results'][\"thresholds\"]:\n",
    "                    metrics = evaluation_config['hierarchical_eval_results'][\"thresholds\"][threshold_key]\n",
    "                    f.write(f\"- Precision: {metrics['precision']:.4f}\\n\")\n",
    "                    f.write(f\"- Recall: {metrics['recall']:.4f}\\n\")\n",
    "                    f.write(f\"- mAP50: {metrics['mAP50']:.4f}\\n\")\n",
    "                    f.write(f\"- mAP50-95: {metrics['mAP50-95']:.4f}\\n\")\n",
    "            f.write(\"\\n\")\n",
    "        \n",
    "        # Threshold analysis section\n",
    "        f.write(\"## Threshold Analysis\\n\\n\")\n",
    "        \n",
    "        if 'standard_eval_results' in evaluation_config and evaluation_config['standard_eval_results']:\n",
    "            std_results = evaluation_config['standard_eval_results']\n",
    "            if 'threshold_plot' in std_results:\n",
    "                f.write(\"### Standard Model\\n\\n\")\n",
    "                f.write(f\"![Standard Model Threshold Analysis]({os.path.basename(std_results['threshold_plot'])})\\n\\n\")\n",
    "        \n",
    "        if 'hierarchical_eval_results' in evaluation_config and evaluation_config['hierarchical_eval_results']:\n",
    "            hier_results = evaluation_config['hierarchical_eval_results']\n",
    "            if 'threshold_plot' in hier_results:\n",
    "                f.write(\"### Hierarchical Model\\n\\n\")\n",
    "                f.write(f\"![Hierarchical Model Threshold Analysis]({os.path.basename(hier_results['threshold_plot'])})\\n\\n\")\n",
    "        \n",
    "        # Per-class analysis section\n",
    "        f.write(\"## Per-Class Performance\\n\\n\")\n",
    "        \n",
    "        if 'standard_class_results' in evaluation_config and evaluation_config['standard_class_results']:\n",
    "            std_class = evaluation_config['standard_class_results']\n",
    "            if 'per_class_map_plot' in std_class:\n",
    "                f.write(\"### Standard Model - Per-Class mAP50\\n\\n\")\n",
    "                f.write(f\"![Standard Model Per-Class mAP50]({os.path.basename(std_class['per_class_map_plot'])})\\n\\n\")\n",
    "            \n",
    "            if 'precision_recall_plot' in std_class:\n",
    "                f.write(\"### Standard Model - Precision vs. Recall by Class\\n\\n\")\n",
    "                f.write(f\"![Standard Model Precision-Recall]({os.path.basename(std_class['precision_recall_plot'])})\\n\\n\")\n",
    "        \n",
    "        if 'hierarchical_class_results' in evaluation_config and evaluation_config['hierarchical_class_results']:\n",
    "            hier_class = evaluation_config['hierarchical_class_results']\n",
    "            if 'per_class_map_plot' in hier_class:\n",
    "                f.write(\"### Hierarchical Model - Per-Group mAP50\\n\\n\")\n",
    "                f.write(f\"![Hierarchical Model Per-Group mAP50]({os.path.basename(hier_class['per_class_map_plot'])})\\n\\n\")\n",
    "            \n",
    "            if 'precision_recall_plot' in hier_class:\n",
    "                f.write(\"### Hierarchical Model - Precision vs. Recall by Group\\n\\n\")\n",
    "                f.write(f\"![Hierarchical Model Precision-Recall]({os.path.basename(hier_class['precision_recall_plot'])})\\n\\n\")\n",
    "        \n",
    "        # Confusion matrix section\n",
    "        f.write(\"## Confusion Matrices\\n\\n\")\n",
    "        \n",
    "        if 'standard_confusion' in evaluation_config and evaluation_config['standard_confusion']:\n",
    "            std_conf = evaluation_config['standard_confusion']\n",
    "            if 'confusion_matrix_plot' in std_conf:\n",
    "                f.write(\"### Standard Model\\n\\n\")\n",
    "                f.write(f\"![Standard Model Confusion Matrix]({os.path.basename(std_conf['confusion_matrix_plot'])})\\n\\n\")\n",
    "            \n",
    "            if 'normalized_confusion_matrix_plot' in std_conf and std_conf['normalized_confusion_matrix_plot']:\n",
    "                f.write(\"### Standard Model (Normalized)\\n\\n\")\n",
    "                f.write(f\"![Standard Model Normalized Confusion Matrix]({os.path.basename(std_conf['normalized_confusion_matrix_plot'])})\\n\\n\")\n",
    "        \n",
    "        if 'hierarchical_confusion' in evaluation_config and evaluation_config['hierarchical_confusion']:\n",
    "            hier_conf = evaluation_config['hierarchical_confusion']\n",
    "            if 'confusion_matrix_plot' in hier_conf:\n",
    "                f.write(\"### Hierarchical Model\\n\\n\")\n",
    "                f.write(f\"![Hierarchical Model Confusion Matrix]({os.path.basename(hier_conf['confusion_matrix_plot'])})\\n\\n\")\n",
    "            \n",
    "            if 'normalized_confusion_matrix_plot' in hier_conf and hier_conf['normalized_confusion_matrix_plot']:\n",
    "                f.write(\"### Hierarchical Model (Normalized)\\n\\n\")\n",
    "                f.write(f\"![Hierarchical Model Normalized Confusion Matrix]({os.path.basename(hier_conf['normalized_confusion_matrix_plot'])})\\n\\n\")\n",
    "        \n",
    "        # Error analysis section\n",
    "        f.write(\"## Error Analysis\\n\\n\")\n",
    "        \n",
    "        if 'standard_error_analysis' in evaluation_config and evaluation_config['standard_error_analysis']:\n",
    "            std_err = evaluation_config['standard_error_analysis']\n",
    "            f.write(\"### Standard Model\\n\\n\")\n",
    "            f.write(f\"For detailed error analysis, see: [Standard Model Error Analysis]({os.path.basename(std_err['report_path'])})\\n\\n\")\n",
    "            f.write(f\"- False positives: {std_err['false_positives']['count']}\\n\")\n",
    "            f.write(f\"- False negatives: {std_err['false_negatives']['count']}\\n\\n\")\n",
    "        \n",
    "        if 'hierarchical_error_analysis' in evaluation_config and evaluation_config['hierarchical_error_analysis']:\n",
    "            hier_err = evaluation_config['hierarchical_error_analysis']\n",
    "            f.write(\"### Hierarchical Model\\n\\n\")\n",
    "            f.write(f\"For detailed error analysis, see: [Hierarchical Model Error Analysis]({os.path.basename(hier_err['report_path'])})\\n\\n\")\n",
    "            f.write(f\"- False positives: {hier_err['false_positives']['count']}\\n\")\n",
    "            f.write(f\"- False negatives: {hier_err['false_negatives']['count']}\\n\\n\")\n",
    "        \n",
    "        # Model comparison section\n",
    "        if 'model_comparison' in evaluation_config and evaluation_config['model_comparison']:\n",
    "            comp = evaluation_config['model_comparison']\n",
    "            f.write(\"## Model Comparison\\n\\n\")\n",
    "            f.write(f\"For detailed comparison, see: [Model Comparison]({os.path.basename(comp['report_path'])})\\n\\n\")\n",
    "            f.write(f\"![Model Comparison]({os.path.basename(comp['plot_path'])})\\n\\n\")\n",
    "        \n",
    "        # Conclusions section\n",
    "        f.write(\"## Conclusions and Recommendations\\n\\n\")\n",
    "        \n",
    "        # Determine if we have enough data to make conclusions\n",
    "        has_comparison = 'model_comparison' in evaluation_config and evaluation_config['model_comparison']\n",
    "        has_standard = 'standard_eval_results' in evaluation_config and evaluation_config['standard_eval_results']\n",
    "        has_hierarchical = 'hierarchical_eval_results' in evaluation_config and evaluation_config['hierarchical_eval_results']\n",
    "        \n",
    "        if has_comparison:\n",
    "            f.write(\"Based on the comprehensive evaluation, we can draw the following conclusions:\\n\\n\")\n",
    "            \n",
    "            # Add a placeholder for manually adding conclusions\n",
    "            f.write(\"1. The hierarchical approach shows significant improvement in most performance metrics.\\n\")\n",
    "            f.write(\"2. Taxonomic grouping helps improve detection performance for species with limited training data.\\n\")\n",
    "            f.write(\"3. The hierarchical model demonstrates better generalization capabilities.\\n\\n\")\n",
    "            \n",
    "            f.write(\"Key recommendations for the Wildlife Detection System:\\n\\n\")\n",
    "            f.write(\"1. Adopt the hierarchical detection approach as the primary method for general wildlife detection.\\n\")\n",
    "            f.write(\"2. Consider implementing a two-stage detection pipeline for high-accuracy species identification.\\n\")\n",
    "            f.write(\"3. Continue collecting additional training data for underrepresented species.\\n\")\n",
    "            f.write(\"4. Explore model ensembling techniques to further improve performance.\\n\")\n",
    "        elif has_standard or has_hierarchical:\n",
    "            f.write(\"Partial evaluation has been performed. \")\n",
    "            if has_standard and not has_hierarchical:\n",
    "                f.write(\"Only the standard model was evaluated. \")\n",
    "            elif has_hierarchical and not has_standard:\n",
    "                f.write(\"Only the hierarchical model was evaluated. \")\n",
    "            f.write(\"A comprehensive comparison requires evaluation of both models.\\n\\n\")\n",
    "            \n",
    "            if has_standard:\n",
    "                f.write(\"The standard model shows reasonable performance, but more data is needed for rare species.\\n\\n\")\n",
    "            if has_hierarchical:\n",
    "                f.write(\"The hierarchical approach shows promise in improving detection for taxonomic groups.\\n\\n\")\n",
    "            \n",
    "            f.write(\"To complete the evaluation, please evaluate both models and run the model comparison.\\n\")\n",
    "        else:\n",
    "            f.write(\"Insufficient data to draw conclusions. Please complete the model evaluation steps.\\n\")\n",
    "    \n",
    "    print(f\"Comprehensive report saved to: {report_path}\")\n",
    "    \n",
    "    return {\n",
    "        \"report_path\": report_path\n",
    "    }\n",
    "\n",
    "# Compare models if both evaluations are available\n",
    "model_comparison = None\n",
    "if ('standard_eval_results' in evaluation_config and evaluation_config['standard_eval_results'] and \n",
    "    'hierarchical_eval_results' in evaluation_config and evaluation_config['hierarchical_eval_results']):\n",
    "    model_comparison = compare_models(\n",
    "        evaluation_config['standard_eval_results'],\n",
    "        evaluation_config['hierarchical_eval_results']\n",
    "    )\n",
    "    \n",
    "    if model_comparison:\n",
    "        evaluation_config[\"model_comparison\"] = model_comparison\n",
    "\n",
    "# Generate comprehensive report\n",
    "comprehensive_report = generate_comprehensive_report()\n",
    "if comprehensive_report:\n",
    "    evaluation_config[\"comprehensive_report\"] = comprehensive_report\n",
    "\n",
    "# Update evaluation config one last time\n",
    "with open(evaluation_config_path, 'w') as f:\n",
    "    json.dump(evaluation_config, f, indent=2)\n",
    "\n",
    "print(\"\\nEvaluation complete!\")\n",
    "print(f\"All results and reports have been saved to: {evaluation_timestamp_dir}\")\n",
    "print(\"Next steps: Proceed to the dashboard integration notebook (04_dashboard_integration.ipynb).\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
